#!/usr/bin/env node

/**
 * Script de migration des fichiers Storage - Version interactive
 * Version: 5.0.0
 *
 * Am√©liorations v5.0.0:
 * - üîß FIX CRITIQUE: R√©√©criture compl√®te de l'ajout PGOPTIONS avec script Python
 * - üìã V√©rification avant/apr√®s modification avec logs d√©taill√©s
 * - üîç Diagnostic complet si √©chec (affiche docker-compose.yml extrait)
 * - ‚úÖ Utilise `docker compose up -d` au lieu de `restart` (recr√©e le conteneur)
 * - üéØ Solution confirm√©e par issue GitHub supabase/storage#383
 *
 * Am√©liorations v4.0.0:
 * - üîÑ Red√©marre TOUS les services Supabase (pas juste Storage)
 * - ‚è±Ô∏è Augmentation d√©lai d'attente 10s ‚Üí 15s (tous les services red√©marrent)
 * - üîß Fix: ALTER DATABASE n√©cessite red√©marrage complet pour prendre effet
 *
 * Am√©liorations v3.8.0:
 * - üîÑ Syst√®me de retry avec 3 tentatives
 * - üìä V√©rification visuelle de l'√©tat du service Storage
 * - ‚úÖ Confirmation visuelle color√©e quand le service est op√©rationnel
 * - üìà Affichage du nombre de tentatives et temps d'attente total
 *
 * Am√©liorations v3.7.0:
 * - üì∫ Affichage des logs d'erreur directement dans le terminal (coloris√©)
 * - üìã Plus besoin de consulter un fichier s√©par√©
 * - üé® Sections claires: Erreur, Diagnostic, Solutions
 *
 * Am√©liorations v3.6.0:
 * - üìù Logs automatiques d√©taill√©s en cas d'erreur
 * - ‚è±Ô∏è Augmentation du d√©lai d'attente apr√®s red√©marrage Storage (1s ‚Üí 10s)
 * - üí° Suggestions de diagnostic et solutions en cas d'√©chec
 *
 * Am√©liorations v3.5.0:
 * - üîß Configure automatiquement le search_path PostgreSQL (storage, public)
 * - üîÑ Red√©marre automatiquement le service Storage apr√®s cr√©ation tables
 * - ‚úÖ L'utilisateur n'a plus besoin d'intervention manuelle
 *
 * Am√©liorations v3.4.0:
 * - üîß Fix PGPASSWORD avec docker exec (-e PGPASSWORD)
 * - üìÅ Utilise fichier SQL temporaire pour √©viter les probl√®mes de heredoc
 *
 * Am√©liorations v3.3.0:
 * - üîß Cr√©ation tables storage via SSH + docker exec (plus fiable)
 * - ‚ö° Plus besoin du package 'pg', utilise SSH directement
 *
 * Am√©liorations v3.2.0:
 * - üîß Installation automatique des d√©pendances npm (@supabase/supabase-js)
 * - ‚ö° Plus besoin de lancer `npm install` manuellement
 *
 * Am√©liorations v3.1.0:
 * - ‚ú® Cr√©ation automatique des tables storage.buckets et storage.objects
 * - üîß D√©tection et r√©solution du probl√®me "relation buckets does not exist"
 * - üöÄ Initialisation automatique du sch√©ma storage si n√©cessaire
 *
 * Am√©liorations v3.0.0:
 * - Interface guid√©e √©tape par √©tape
 * - Test automatique avant migration r√©elle
 * - Barre de progression
 * - R√©sum√© visuel am√©lior√©
 * - Confirmation √† chaque √©tape
 *
 * S√©curit√©s v2.0.0:
 * - Pagination (support > 1000 fichiers)
 * - Retry automatique (3 tentatives)
 * - Validation taille fichiers (max 100MB)
 * - Timeout upload/download (5min)
 * - Log d√©taill√© des erreurs
 * - Manifest JSON
 *
 * Pr√©requis:
 *   npm install @supabase/supabase-js
 *   SSH configur√© vers le Pi (ssh pi@IP_DU_PI)
 *
 * Usage:
 *   node post-migration-storage.js [--max-size=50] [--skip-test]
 *
 * Options:
 *   --max-size=N    Taille max en MB (d√©faut: 100)
 *   --skip-test     Sauter l'√©tape de test (non recommand√©)
 */

const readline = require('readline');
const fs = require('fs').promises;
const { execSync } = require('child_process');

// V√©rification et installation automatique des d√©pendances
function checkAndInstallDependencies() {
  const dependencies = ['@supabase/supabase-js'];
  const missing = [];

  for (const dep of dependencies) {
    try {
      require.resolve(dep);
    } catch {
      missing.push(dep);
    }
  }

  if (missing.length > 0) {
    console.log(`\n‚ö†Ô∏è  D√©pendances manquantes d√©tect√©es: ${missing.join(', ')}`);
    console.log(`üì¶ Installation automatique en cours...\n`);

    try {
      execSync(`npm install ${missing.join(' ')}`, { stdio: 'inherit' });
      console.log(`\n‚úÖ D√©pendances install√©es avec succ√®s!\n`);
    } catch (error) {
      console.error(`\n‚ùå √âchec installation des d√©pendances.`);
      console.error(`   Installez-les manuellement: npm install ${missing.join(' ')}\n`);
      process.exit(1);
    }
  }
}

// V√©rifier les d√©pendances au d√©marrage
checkAndInstallDependencies();

const MAX_SIZE_MB = parseInt(process.argv.find(arg => arg.startsWith('--max-size='))?.split('=')[1] || '100');
const MAX_SIZE_BYTES = MAX_SIZE_MB * 1024 * 1024;
const SKIP_TEST = process.argv.includes('--skip-test');
const RETRY_COUNT = 3;
const TIMEOUT_MS = 5 * 60 * 1000;
const PAGINATION_LIMIT = 100;

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

function question(query) {
  return new Promise(resolve => rl.question(query, resolve));
}

// Couleurs console
const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  cyan: '\x1b[36m'
};

function printStep(num, total, title) {
  console.log(`\n${colors.cyan}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${colors.reset}`);
  console.log(`${colors.cyan}‚ïë${colors.reset} ${colors.bright}√âTAPE ${num}/${total}: ${title.padEnd(42)}${colors.reset}${colors.cyan}‚ïë${colors.reset}`);
  console.log(`${colors.cyan}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${colors.reset}\n`);
}

function printSuccess(message) {
  console.log(`${colors.green}‚úÖ ${message}${colors.reset}`);
}

function printWarning(message) {
  console.log(`${colors.yellow}‚ö†Ô∏è  ${message}${colors.reset}`);
}

function printInfo(message) {
  console.log(`${colors.blue}‚Ñπ  ${message}${colors.reset}`);
}

// Timeout promise
function withTimeout(promise, ms) {
  return Promise.race([
    promise,
    new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), ms))
  ]);
}

// Retry avec backoff exponentiel
async function retryWithBackoff(fn, retries = RETRY_COUNT) {
  for (let i = 0; i < retries; i++) {
    try {
      return await fn();
    } catch (err) {
      if (i === retries - 1) throw err;
      const delay = Math.pow(2, i) * 1000;
      console.log(`    ‚è±Ô∏è  Retry ${i + 1}/${retries} dans ${delay}ms...`);
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}

// Lister tous les fichiers avec pagination
async function listAllFiles(client, bucketName) {
  let allFiles = [];
  let offset = 0;
  let hasMore = true;

  while (hasMore) {
    const { data: files, error } = await client.storage
      .from(bucketName)
      .list('', {
        limit: PAGINATION_LIMIT,
        offset: offset,
        sortBy: { column: 'name', order: 'asc' }
      });

    if (error) throw error;

    if (files && files.length > 0) {
      allFiles = allFiles.concat(files);
      offset += files.length;
      hasMore = files.length === PAGINATION_LIMIT;
    } else {
      hasMore = false;
    }
  }

  return allFiles;
}

// Barre de progression
function printProgress(current, total, fileName) {
  const percent = Math.round((current / total) * 100);
  const barLength = 30;
  const filled = Math.round((barLength * current) / total);
  const bar = '‚ñà'.repeat(filled) + '‚ñë'.repeat(barLength - filled);

  process.stdout.write(`\r  [${bar}] ${percent}% (${current}/${total}) ${fileName.substring(0, 30)}...`);

  if (current === total) {
    process.stdout.write('\n');
  }
}

function formatBytes(bytes) {
  if (bytes === 0) return '0 Bytes';
  const k = 1024;
  const sizes = ['Bytes', 'KB', 'MB', 'GB'];
  const i = Math.floor(Math.log(bytes) / Math.log(k));
  return Math.round(bytes / Math.pow(k, i) * 100) / 100 + ' ' + sizes[i];
}

async function testConnection(cloudClient, piClient, piUrl, piServiceKey) {
  printStep(1, SKIP_TEST ? 5 : 6, 'Test de connexion');

  printInfo('Test connexion Cloud...');
  try {
    const { data, error } = await cloudClient.storage.listBuckets();
    if (error) throw error;
    printSuccess(`Cloud connect√© (${data.length} buckets d√©tect√©s)`);
  } catch (err) {
    console.error(`\n‚ùå Erreur connexion Cloud: ${err.message}`);
    console.error('   V√©rifiez votre Service Role Key Cloud\n');
    return false;
  }

  printInfo('Test connexion Pi...');

  // Try to list buckets first - if it fails, we need to create the storage tables
  let needsStorageTables = false;
  try {
    const { data, error } = await piClient.storage.listBuckets();
    if (error) {
      // Check if error is about missing tables
      if (error.message.includes('relation') || error.message.includes('does not exist')) {
        needsStorageTables = true;
        printWarning('Tables storage.buckets/objects non d√©tect√©es');
      } else {
        throw error;
      }
    } else {
      printSuccess(`Pi connect√© (${data.length} buckets existants)`);
    }
  } catch (err) {
    console.error(`\n‚ùå Erreur connexion Pi: ${err.message}`);
    console.error('   V√©rifiez votre URL Pi et Service Role Key\n');
    return false;
  }

  // Parse Pi URL to get hostname (needed for error reporting too)
  const piUrlObj = new URL(piUrl);
  const piHost = piUrlObj.hostname;

  // Create storage tables if needed
  if (needsStorageTables) {
    printInfo('Cr√©ation automatique des tables storage via SSH...');

    try {
      const { execSync } = require('child_process');

      // Ask for PostgreSQL password
      const pgPassword = await question('  Mot de passe PostgreSQL (POSTGRES_PASSWORD du Pi): ');

      printInfo('Connexion SSH au Pi...');

      // Create SQL commands
      const sqlCommands = `-- Cr√©er le sch√©ma storage
CREATE SCHEMA IF NOT EXISTS storage;

-- Cr√©er la table buckets
CREATE TABLE IF NOT EXISTS storage.buckets (
  id text PRIMARY KEY,
  name text NOT NULL UNIQUE,
  owner uuid,
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now(),
  public boolean DEFAULT false,
  avif_autodetection boolean DEFAULT false,
  file_size_limit bigint,
  allowed_mime_types text[]
);

ALTER TABLE storage.buckets ENABLE ROW LEVEL SECURITY;

GRANT ALL ON storage.buckets TO postgres, service_role;
GRANT SELECT ON storage.buckets TO anon, authenticated;

-- Cr√©er la table objects
CREATE TABLE IF NOT EXISTS storage.objects (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  bucket_id text REFERENCES storage.buckets(id),
  name text,
  owner uuid,
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now(),
  last_accessed_at timestamptz DEFAULT now(),
  metadata jsonb,
  path_tokens text[] GENERATED ALWAYS AS (string_to_array(name, '/')) STORED,
  version text,
  UNIQUE(bucket_id, name)
);

ALTER TABLE storage.objects ENABLE ROW LEVEL SECURITY;

CREATE INDEX IF NOT EXISTS objects_bucket_id_idx ON storage.objects(bucket_id);
CREATE INDEX IF NOT EXISTS objects_name_idx ON storage.objects(name);
CREATE INDEX IF NOT EXISTS objects_owner_idx ON storage.objects(owner);

GRANT ALL ON storage.objects TO postgres, service_role;
GRANT SELECT ON storage.objects TO anon, authenticated;

-- Configurer le search_path pour que l'API Storage trouve les tables
ALTER DATABASE postgres SET search_path TO storage, public;

SELECT 'Tables cr√©√©es' as status;`;

      // Write SQL to temp file
      const tmpFile = '/tmp/supabase-storage-init.sql';
      await fs.writeFile(tmpFile, sqlCommands);

      // Execute via SSH with password in docker exec environment
      const sshCommand = `ssh pi@${piHost} "docker exec -i -e PGPASSWORD='${pgPassword}' supabase-db psql -U postgres -d postgres" < ${tmpFile}`;

      execSync(sshCommand, { stdio: 'inherit' });

      // Clean up
      await fs.unlink(tmpFile);

      printSuccess('Tables storage cr√©√©es avec succ√®s');

      printInfo('Configuration de PGOPTIONS dans le service Storage...');

      // V√©rifier si PGOPTIONS existe d√©j√†
      const checkPgOptionsCmd = `ssh pi@${piHost} "grep 'PGOPTIONS.*search_path' ~/stacks/supabase/docker-compose.yml"`;
      let pgOptionsExists = false;

      try {
        execSync(checkPgOptionsCmd, { stdio: 'pipe' });
        pgOptionsExists = true;
        printSuccess('PGOPTIONS d√©j√† pr√©sent dans docker-compose.yml');
      } catch (err) {
        // PGOPTIONS n'existe pas, on va l'ajouter
        printWarning('PGOPTIONS absent, ajout automatique...');

        // Cr√©er un script Python temporaire pour modifier docker-compose.yml de fa√ßon robuste
        const pythonScript = `
import re
import sys

# Lire le fichier
with open('/home/pi/stacks/supabase/docker-compose.yml', 'r') as f:
    content = f.read()

# V√©rifier si PGOPTIONS existe d√©j√†
if 'PGOPTIONS' in content:
    print("PGOPTIONS_ALREADY_EXISTS")
    sys.exit(0)

# Pattern pour trouver la section storage et ajouter PGOPTIONS apr√®s DATABASE_URL
pattern = r'(container_name: supabase-storage.*?environment:.*?DATABASE_URL: [^\\n]+)'
replacement = r'\\1\\n      PGOPTIONS: "-c search_path=storage,public"'

# Appliquer le remplacement
new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)

# V√©rifier que la modification a √©t√© faite
if new_content != content:
    # Sauvegarder
    with open('/home/pi/stacks/supabase/docker-compose.yml', 'w') as f:
        f.write(new_content)
    print("PGOPTIONS_ADDED_SUCCESS")
else:
    print("PGOPTIONS_ADD_FAILED")
    sys.exit(1)
`;

        // √âcrire le script Python sur le Pi
        const tmpPythonFile = '/tmp/add_pgoptions.py';
        const writePythonCmd = `ssh pi@${piHost} "cat > ${tmpPythonFile}" <<'PYTHON_SCRIPT_EOF'
${pythonScript}
PYTHON_SCRIPT_EOF`;

        try {
          execSync(writePythonCmd, { stdio: 'pipe' });

          // Ex√©cuter le script Python
          const runPythonCmd = `ssh pi@${piHost} "python3 ${tmpPythonFile}"`;
          const pythonOutput = execSync(runPythonCmd, { encoding: 'utf8' }).trim();

          if (pythonOutput === 'PGOPTIONS_ALREADY_EXISTS') {
            printSuccess('PGOPTIONS d√©j√† pr√©sent (d√©tect√© par Python)');
            pgOptionsExists = true;
          } else if (pythonOutput === 'PGOPTIONS_ADDED_SUCCESS') {
            printSuccess('PGOPTIONS ajout√© avec succ√®s dans docker-compose.yml');
            pgOptionsExists = true;
          } else {
            throw new Error('√âchec ajout PGOPTIONS: ' + pythonOutput);
          }

          // Nettoyer le fichier temporaire
          execSync(`ssh pi@${piHost} "rm -f ${tmpPythonFile}"`, { stdio: 'pipe' });

        } catch (err) {
          printError('√âchec modification docker-compose.yml avec Python');
          printWarning('Affichage de la section storage actuelle:');

          // Afficher la section storage pour diagnostic
          const showStorageCmd = `ssh pi@${piHost} "sed -n '/container_name: supabase-storage/,/healthcheck:/p' ~/stacks/supabase/docker-compose.yml"`;
          const storageSection = execSync(showStorageCmd, { encoding: 'utf8' });
          console.log('\n' + colors.cyan + storageSection + colors.reset);

          throw new Error("Impossible d'ajouter PGOPTIONS automatiquement. Modifiez manuellement docker-compose.yml");
        }
      }

      // V√©rification post-modification
      printInfo('V√©rification finale de PGOPTIONS...');
      const verifyCmd = `ssh pi@${piHost} "grep -A 2 'DATABASE_URL.*postgres' ~/stacks/supabase/docker-compose.yml | grep PGOPTIONS"`;

      try {
        const verifyOutput = execSync(verifyCmd, { encoding: 'utf8' }).trim();
        printSuccess('‚úì V√©rification OK: ' + verifyOutput);
      } catch (err) {
        printError('‚úó PGOPTIONS non d√©tect√© apr√®s modification!');
        throw new Error('√âchec v√©rification PGOPTIONS');
      }

      printWarning('Red√©marrage de TOUS les services Supabase avec recr√©ation des conteneurs...');
      printInfo('(Utilisation de "docker compose up -d" pour appliquer les nouvelles variables)');

      // Use docker compose up -d to recreate containers with new environment variables
      const upCommand = `ssh pi@${piHost} "cd ~/stacks/supabase && docker compose up -d storage"`;
      execSync(upCommand, { stdio: 'pipe' });

      printSuccess('Service Storage recr√©√© avec PGOPTIONS');

      // Wait for ALL services to be fully ready with retry mechanism
      const MAX_RETRIES = 3;
      const WAIT_TIME = 15000; // 15 seconds (longer because all services restart)
      let retryCount = 0;
      let storageReady = false;

      while (retryCount < MAX_RETRIES && !storageReady) {
        retryCount++;

        if (retryCount === 1) {
          printInfo(`Attente du red√©marrage du service Storage (${WAIT_TIME/1000}s)...`);
        } else {
          printWarning(`Tentative ${retryCount}/${MAX_RETRIES} - Nouvelle attente de ${WAIT_TIME/1000}s...`);
        }

        await new Promise(resolve => setTimeout(resolve, WAIT_TIME));

        // Verify storage API
        printInfo(`V√©rification de l'API Storage (tentative ${retryCount}/${MAX_RETRIES})...`);

        try {
          const { data, error } = await piClient.storage.listBuckets();

          if (error) {
            console.error(`   ‚ö†Ô∏è  Erreur: ${error.message}`);

            if (retryCount < MAX_RETRIES) {
              printWarning(`Le service n'est pas encore pr√™t, nouvelle tentative...`);
            } else {
              throw error;
            }
          } else {
            storageReady = true;
            printSuccess(`‚úÖ Pi Storage API accessible ! (${data.length} buckets d√©tect√©s)`);

            // Display visual confirmation
            console.log('\n' + colors.green + '‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó' + colors.reset);
            console.log(colors.green + '‚ïë  ‚úÖ SERVICE STORAGE OP√âRATIONNEL                  ‚ïë' + colors.reset);
            console.log(colors.green + '‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù' + colors.reset);
            console.log(colors.bright + `  ‚Ä¢ Buckets d√©tect√©s: ${data.length}` + colors.reset);
            console.log(colors.bright + `  ‚Ä¢ Tentatives n√©cessaires: ${retryCount}/${MAX_RETRIES}` + colors.reset);
            console.log(colors.bright + `  ‚Ä¢ Temps d'attente total: ${(retryCount * WAIT_TIME) / 1000}s\n` + colors.reset);
          }
        } catch (verifyErr) {
          if (retryCount >= MAX_RETRIES) {
            throw verifyErr;
          }
        }
      }

    } catch (err) {
      // Display detailed error log directly in terminal
      console.error('\n' + colors.red + '‚ïê'.repeat(60) + colors.reset);
      console.error(colors.red + '  ‚ùå ERREUR MIGRATION STORAGE' + colors.reset);
      console.error(colors.red + '‚ïê'.repeat(60) + colors.reset + '\n');

      console.error(colors.bright + 'üìÖ Date:' + colors.reset + ' ' + new Date().toISOString());
      console.error(colors.bright + 'üí¨ Message:' + colors.reset + ' ' + err.message);

      console.error('\n' + colors.bright + 'üìç Configuration:' + colors.reset);
      console.error('  - Pi Host: ' + piHost);
      console.error('  - Pi URL: ' + piUrl);

      console.error('\n' + colors.bright + 'üîç Stack Trace:' + colors.reset);
      console.error(colors.dim + err.stack + colors.reset);

      console.error('\n' + colors.yellow + '‚ïê'.repeat(60) + colors.reset);
      console.error(colors.yellow + '  üîß COMMANDES DE DIAGNOSTIC' + colors.reset);
      console.error(colors.yellow + '‚ïê'.repeat(60) + colors.reset + '\n');

      console.error(colors.bright + '1. V√©rifier SSH:' + colors.reset);
      console.error(colors.cyan + `   ssh pi@${piHost} "echo OK"` + colors.reset);

      console.error('\n' + colors.bright + '2. V√©rifier tables PostgreSQL:' + colors.reset);
      console.error(colors.cyan + `   ssh pi@${piHost} "docker exec supabase-db psql -U postgres -d postgres -c '\\\\dt storage.*'"` + colors.reset);

      console.error('\n' + colors.bright + '3. V√©rifier search_path:' + colors.reset);
      console.error(colors.cyan + `   ssh pi@${piHost} "docker exec supabase-db psql -U postgres -d postgres -c 'SHOW search_path;'"` + colors.reset);

      console.error('\n' + colors.bright + '4. V√©rifier logs Storage:' + colors.reset);
      console.error(colors.cyan + `   ssh pi@${piHost} "docker logs supabase-storage --tail 50"` + colors.reset);

      console.error('\n' + colors.bright + '5. Red√©marrer tous les services Supabase:' + colors.reset);
      console.error(colors.cyan + `   ssh pi@${piHost} "cd ~/stacks/supabase && docker compose restart"` + colors.reset);

      console.error('\n' + colors.green + '‚ïê'.repeat(60) + colors.reset);
      console.error(colors.green + '  üí° SOLUTIONS POSSIBLES' + colors.reset);
      console.error(colors.green + '‚ïê'.repeat(60) + colors.reset + '\n');

      console.error('  1. Le service Storage met ~10s √† red√©marrer ‚Üí Attendez et relancez le script');
      console.error('  2. V√©rifiez la connexion SSH: ' + colors.cyan + `ssh pi@${piHost}` + colors.reset);
      console.error('  3. Relancez tous les services: Utilisez la commande 5 ci-dessus\n');

      return false;
    }
  }

  return true;
}

async function analyzeBuckets(cloudClient) {
  printStep(2, SKIP_TEST ? 5 : 6, 'Analyse des buckets Cloud');

  const { data: buckets, error } = await cloudClient.storage.listBuckets();
  if (error) throw error;

  console.log(`\nüì¶ ${colors.bright}${buckets.length} buckets trouv√©s${colors.reset}:\n`);

  const analysis = [];
  let totalFiles = 0;
  let totalSize = 0;

  for (const bucket of buckets) {
    printInfo(`Analyse ${bucket.name}...`);
    const files = await listAllFiles(cloudClient, bucket.name);

    // Filtrer les vrais fichiers (ignore dossiers et fichiers vides)
    const realFiles = files.filter(f => f.metadata?.size > 0);
    const bucketSize = realFiles.reduce((sum, f) => sum + (f.metadata?.size || 0), 0);

    totalFiles += realFiles.length;
    totalSize += bucketSize;

    console.log(`  ‚îî‚îÄ ${bucket.name}: ${realFiles.length} fichiers (${formatBytes(bucketSize)})`);

    analysis.push({
      bucket,
      files: realFiles,
      size: bucketSize
    });
  }

  console.log(`\n${colors.bright}R√©sum√©:${colors.reset}`);
  console.log(`  ‚Ä¢ Total: ${totalFiles} fichiers`);
  console.log(`  ‚Ä¢ Taille: ${formatBytes(totalSize)}`);
  console.log(`  ‚Ä¢ Buckets: ${buckets.length}\n`);

  return analysis;
}

async function performDryRun(cloudClient, analysis) {
  printStep(3, 6, 'Test de t√©l√©chargement (dry-run)');

  printWarning('Test sans upload sur le Pi (v√©rification acc√®s fichiers)');
  console.log();

  let successCount = 0;
  let errorCount = 0;
  const errors = [];

  for (const { bucket, files } of analysis) {
    if (files.length === 0) continue;

    console.log(`\nüì¶ ${bucket.name} (${files.length} fichiers)`);

    for (let i = 0; i < files.length; i++) {
      const file = files[i];
      const fileSize = file.metadata?.size || 0;

      if (fileSize > MAX_SIZE_BYTES) {
        printWarning(`${file.name}: Trop gros (${formatBytes(fileSize)})`);
        errorCount++;
        errors.push({ bucket: bucket.name, file: file.name, error: 'File too large' });
        continue;
      }

      try {
        const { data, error } = await retryWithBackoff(async () => {
          return await withTimeout(
            cloudClient.storage.from(bucket.name).download(file.name),
            TIMEOUT_MS
          );
        });

        if (error || !data) {
          console.log(`  ‚ùå ${file.name}`);
          errorCount++;
          errors.push({ bucket: bucket.name, file: file.name, error: error?.message || 'Unknown' });
        } else {
          console.log(`  ‚úÖ ${file.name} (${formatBytes(fileSize)})`);
          successCount++;
        }
      } catch (err) {
        console.log(`  ‚ùå ${file.name}: ${err.message}`);
        errorCount++;
        errors.push({ bucket: bucket.name, file: file.name, error: err.message });
      }
    }
  }

  console.log(`\n${colors.bright}R√©sultat du test:${colors.reset}`);
  console.log(`  ‚úÖ T√©l√©chargeables: ${successCount}`);
  console.log(`  ‚ùå Erreurs: ${errorCount}\n`);

  if (errorCount > 0) {
    printWarning(`${errorCount} fichiers ne peuvent pas √™tre t√©l√©charg√©s`);
    printInfo('Ces fichiers seront ignor√©s pendant la migration\n');
  }

  return { successCount, errorCount, errors };
}

async function performMigration(cloudClient, piClient, analysis, testResults) {
  printStep(SKIP_TEST ? 3 : 4, SKIP_TEST ? 5 : 6, 'Migration des fichiers');

  const confirm = await question(`\n${colors.yellow}‚ö†Ô∏è  Lancer la migration r√©elle de ${testResults.successCount} fichiers ? (y/n): ${colors.reset}`);

  if (confirm.toLowerCase() !== 'y') {
    printWarning('Migration annul√©e par l\'utilisateur');
    return null;
  }

  console.log();
  let successCount = 0;
  let errorCount = 0;
  let skippedCount = 0;
  const errorLog = [];
  const manifest = [];
  let fileIndex = 0;

  const totalFiles = testResults.successCount;

  for (const { bucket, files } of analysis) {
    if (files.length === 0) continue;

    // Cr√©er bucket sur Pi
    const { error: createError } = await piClient.storage.createBucket(bucket.name, {
      public: bucket.public
    });

    if (createError && !createError.message.includes('already exists')) {
      printWarning(`Erreur cr√©ation bucket ${bucket.name}`);
      continue;
    }

    for (const file of files) {
      fileIndex++;
      const fileSize = file.metadata?.size || 0;

      // Skip si trop gros
      if (fileSize > MAX_SIZE_BYTES) {
        skippedCount++;
        continue;
      }

      printProgress(fileIndex, totalFiles, file.name);

      try {
        // Download
        const fileData = await retryWithBackoff(async () => {
          return await withTimeout(
            cloudClient.storage.from(bucket.name).download(file.name),
            TIMEOUT_MS
          );
        });

        if (fileData.error || !fileData.data) {
          errorCount++;
          errorLog.push({ bucket: bucket.name, file: file.name, error: fileData.error?.message });
          continue;
        }

        // Upload
        const uploadResult = await retryWithBackoff(async () => {
          return await withTimeout(
            piClient.storage.from(bucket.name).upload(file.name, fileData.data, {
              contentType: file.metadata?.mimetype,
              upsert: true
            }),
            TIMEOUT_MS
          );
        });

        if (uploadResult.error) {
          errorCount++;
          errorLog.push({ bucket: bucket.name, file: file.name, error: uploadResult.error.message });
        } else {
          successCount++;
          manifest.push({
            bucket: bucket.name,
            file: file.name,
            size: fileSize,
            mimetype: file.metadata?.mimetype
          });
        }

        await new Promise(resolve => setTimeout(resolve, 100));

      } catch (err) {
        errorCount++;
        errorLog.push({ bucket: bucket.name, file: file.name, error: err.message });
      }
    }
  }

  return { successCount, errorCount, skippedCount, errorLog, manifest };
}

async function main() {
  console.clear();
  console.log(`\n${colors.cyan}${'‚ïê'.repeat(60)}${colors.reset}`);
  console.log(`${colors.bright}  üì¶ Migration Storage Supabase Cloud ‚Üí Pi (v5.0.0)${colors.reset}`);
  console.log(`${colors.cyan}${'‚ïê'.repeat(60)}${colors.reset}\n`);

  printInfo(`Configuration: Taille max ${MAX_SIZE_MB}MB ‚Ä¢ Timeout ${TIMEOUT_MS/1000}s ‚Ä¢ ${RETRY_COUNT} retries\n`);

  // √âTAPE 0: Configuration
  printStep(0, SKIP_TEST ? 5 : 6, 'Configuration');

  console.log('üìã Configuration Supabase Cloud (source):\n');
  const cloudUrl = await question('  URL Cloud (ex: https://xxxxx.supabase.co): ');
  const cloudServiceKey = await question('  Service Role Key Cloud: ');

  console.log('\nüìã Configuration Supabase Pi (destination):\n');
  const piUrl = await question('  URL Pi (ex: http://192.168.1.74:8001): ');
  const piServiceKey = await question('  Service Role Key Pi (SUPABASE_SERVICE_KEY): ');

  // Import Supabase
  const { createClient } = await import('@supabase/supabase-js');
  const cloudClient = createClient(cloudUrl, cloudServiceKey);
  const piClient = createClient(piUrl, piServiceKey);

  // √âTAPE 1: Test connexion
  const connected = await testConnection(cloudClient, piClient, piUrl, piServiceKey);
  if (!connected) {
    rl.close();
    process.exit(1);
  }

  const proceed1 = await question(`\n${colors.green}‚úì Connexions OK. Continuer ? (y/n): ${colors.reset}`);
  if (proceed1.toLowerCase() !== 'y') {
    printWarning('Migration annul√©e');
    rl.close();
    return;
  }

  // √âTAPE 2: Analyse
  const analysis = await analyzeBuckets(cloudClient);

  const proceed2 = await question(`\n${colors.green}‚úì Analyse termin√©e. Continuer ? (y/n): ${colors.reset}`);
  if (proceed2.toLowerCase() !== 'y') {
    printWarning('Migration annul√©e');
    rl.close();
    return;
  }

  // √âTAPE 3: Dry-run (optionnel)
  let testResults = null;
  if (!SKIP_TEST) {
    testResults = await performDryRun(cloudClient, analysis);

    if (testResults.errorCount > 0) {
      const proceed3 = await question(`\n${colors.yellow}‚ö†Ô∏è  ${testResults.errorCount} fichiers avec erreurs. Continuer quand m√™me ? (y/n): ${colors.reset}`);
      if (proceed3.toLowerCase() !== 'y') {
        printWarning('Migration annul√©e');
        rl.close();
        return;
      }
    } else {
      const proceed3 = await question(`\n${colors.green}‚úì Tous les fichiers sont accessibles. Lancer la migration ? (y/n): ${colors.reset}`);
      if (proceed3.toLowerCase() !== 'y') {
        printWarning('Migration annul√©e');
        rl.close();
        return;
      }
    }
  } else {
    const totalFiles = analysis.reduce((sum, a) => sum + a.files.length, 0);
    testResults = { successCount: totalFiles, errorCount: 0, errors: [] };
  }

  // √âTAPE 4/5: Migration
  const startTime = Date.now();
  const migrationResults = await performMigration(cloudClient, piClient, analysis, testResults);

  if (!migrationResults) {
    rl.close();
    return;
  }

  const duration = Math.round((Date.now() - startTime) / 1000);

  // √âTAPE 5/6: Sauvegarde manifest
  printStep(SKIP_TEST ? 4 : 5, SKIP_TEST ? 5 : 6, 'Sauvegarde du rapport');

  const manifestPath = `storage-migration-${Date.now()}.json`;
  await fs.writeFile(manifestPath, JSON.stringify({
    timestamp: new Date().toISOString(),
    maxSizeMB: MAX_SIZE_MB,
    stats: {
      total: testResults.successCount,
      success: migrationResults.successCount,
      errors: migrationResults.errorCount,
      skipped: migrationResults.skippedCount,
      duration: duration
    },
    files: migrationResults.manifest,
    errors: migrationResults.errorLog
  }, null, 2));

  printSuccess(`Rapport sauvegard√©: ${manifestPath}`);

  // √âTAPE 6: R√©sum√© final
  printStep(SKIP_TEST ? 5 : 6, SKIP_TEST ? 5 : 6, 'R√©sum√© final');

  console.log(`\n${colors.cyan}${'‚ïê'.repeat(60)}${colors.reset}`);
  console.log(`${colors.bright}  üéâ MIGRATION TERMIN√âE${colors.reset}`);
  console.log(`${colors.cyan}${'‚ïê'.repeat(60)}${colors.reset}\n`);

  console.log(`${colors.green}‚úÖ Succ√®s:${colors.reset}   ${migrationResults.successCount}/${testResults.successCount} fichiers`);
  console.log(`${colors.yellow}‚ùå Erreurs:${colors.reset}  ${migrationResults.errorCount} fichiers`);
  console.log(`${colors.blue}‚è≠Ô∏è  Ignor√©s:${colors.reset}  ${migrationResults.skippedCount} fichiers (trop gros)`);
  console.log(`${colors.cyan}‚è±Ô∏è  Dur√©e:${colors.reset}    ${duration}s\n`);

  if (migrationResults.errorCount > 0) {
    printWarning(`Consultez ${manifestPath} pour les d√©tails des erreurs`);
  }

  console.log(`${colors.cyan}${'‚ïê'.repeat(60)}${colors.reset}\n`);

  rl.close();
}

main().catch(err => {
  console.error(`\n${colors.yellow}‚ùå Erreur fatale:${colors.reset} ${err.message}\n`);
  rl.close();
  process.exit(1);
});
