#!/bin/bash
# =============================================================================
# Ollama + Open WebUI Deployment - Local LLM (ChatGPT Alternative)
# =============================================================================
# Version: 1.1.0
# Last updated: 2025-01-14
# Author: PI5-SETUP Project
# Usage: sudo bash 01-ollama-deploy.sh
# =============================================================================
# Sources officielles :
# - Ollama: https://github.com/ollama/ollama
# - Open WebUI: https://github.com/open-webui/open-webui
# Ce script est IDEMPOTENT
# =============================================================================

set -euo pipefail

# === Logging functions ===
log_info() { echo -e "\033[0;34m[INFO]\033[0m $*"; }
log_error() { echo -e "\033[0;31m[ERROR]\033[0m $*" >&2; }
log_success() { echo -e "\033[0;32m[SUCCESS]\033[0m $*"; }
log_warn() { echo -e "\033[0;33m[WARN]\033[0m $*"; }

# === Detect current user ===
CURRENT_USER="${SUDO_USER:-$(whoami)}"
USER_HOME=$(eval echo "~${CURRENT_USER}")

# === Configuration ===
STACK_NAME="ollama"
STACK_DIR="${USER_HOME}/stacks/${STACK_NAME}"
COMPOSE_FILE="${STACK_DIR}/docker-compose.yml"
ENV_FILE="${STACK_DIR}/.env"
MODELS_DIR="${USER_HOME}/data/ollama/models"

TRAEFIK_ENV="${USER_HOME}/stacks/traefik/.env"
TRAEFIK_SCENARIO="none"

# === Check root ===
if [[ $EUID -ne 0 ]]; then
    log_error "Ce script doit Ãªtre lancÃ© avec sudo"
    exit 1
fi

#######################
# FONCTIONS
#######################

check_requirements() {
    log_info "VÃ©rification configuration requise..."

    # Check RAM (minimum 8GB recommandÃ©)
    local total_ram
    total_ram=$(free -g | awk '/^Mem:/{print $2}')

    if [[ ${total_ram} -lt 8 ]]; then
        log_warn "RAM dÃ©tectÃ©e : ${total_ram}GB (recommandÃ©: 8GB min)"
        log_warn "Utilisez des modÃ¨les lÃ©gers (phi3:3.8b, tinyllama:1.1b)"
    else
        log_success "RAM : ${total_ram}GB âœ“"
    fi

    # Check architecture
    local arch
    arch=$(uname -m)
    if [[ "${arch}" != "aarch64" ]] && [[ "${arch}" != "arm64" ]] && [[ "${arch}" != "x86_64" ]]; then
        log_error "Architecture non supportÃ©e : ${arch}"
        exit 1
    fi
    log_success "Architecture : ${arch} âœ“"
}

detect_traefik_scenario() {
    [[ ! -f "${TRAEFIK_ENV}" ]] && return

    if grep -q "DUCKDNS_SUBDOMAIN" "${TRAEFIK_ENV}" 2>/dev/null; then
        TRAEFIK_SCENARIO="duckdns"
        DUCKDNS_SUBDOMAIN=$(grep "^DUCKDNS_SUBDOMAIN=" "${TRAEFIK_ENV}" | cut -d'=' -f2)
    elif grep -q "CLOUDFLARE_API_TOKEN" "${TRAEFIK_ENV}" 2>/dev/null; then
        TRAEFIK_SCENARIO="cloudflare"
        DOMAIN=$(grep "^DOMAIN=" "${TRAEFIK_ENV}" | cut -d'=' -f2)
    elif grep -q "VPN_MODE" "${TRAEFIK_ENV}" 2>/dev/null; then
        TRAEFIK_SCENARIO="vpn"
    fi
}

create_env() {
    cat > "${ENV_FILE}" <<EOF
# Ollama + Open WebUI Configuration
# GÃ©nÃ©rÃ© le $(date)

# Ollama
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_MODELS=${MODELS_DIR}

# Open WebUI
WEBUI_SECRET_KEY=$(openssl rand -hex 32)
WEBUI_NAME="Pi5 AI Chat"

# Timezone
TZ=Europe/Paris
EOF

    chmod 600 "${ENV_FILE}"
}

create_compose() {
    cat > "${COMPOSE_FILE}" <<'EOF'
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    volumes:
      - ${OLLAMA_MODELS}:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    volumes:
      - ./data:/app/backend/data
    environment:
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - WEBUI_NAME=${WEBUI_NAME}
      - OLLAMA_BASE_URL=http://ollama:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
EOF

    # Ajouter Traefik si dÃ©tectÃ© et rÃ©seau existe
    if [[ "${TRAEFIK_SCENARIO}" != "none" ]] && docker network ls | grep -q "traefik_network"; then
        cat >> "${COMPOSE_FILE}" <<EOF
    networks:
      - default
      - traefik_network
    labels:
      - "traefik.enable=true"
      - "traefik.http.services.ollama-ui.loadbalancer.server.port=8080"
EOF

        case "${TRAEFIK_SCENARIO}" in
            duckdns)
                cat >> "${COMPOSE_FILE}" <<EOF
      - "traefik.http.routers.ollama-ui.rule=Host(\`ai.${DUCKDNS_SUBDOMAIN}.duckdns.org\`)"
      - "traefik.http.routers.ollama-ui.entrypoints=websecure"
      - "traefik.http.routers.ollama-ui.tls.certresolver=letsencrypt"
EOF
                ;;
            cloudflare)
                cat >> "${COMPOSE_FILE}" <<EOF
      - "traefik.http.routers.ollama-ui.rule=Host(\`ai.${DOMAIN}\`)"
      - "traefik.http.routers.ollama-ui.entrypoints=websecure"
      - "traefik.http.routers.ollama-ui.tls.certresolver=letsencrypt"
EOF
                ;;
        esac

        cat >> "${COMPOSE_FILE}" <<'EOF'

networks:
  default:
    name: ollama_network
  traefik_network:
    external: true
EOF
    else
        cat >> "${COMPOSE_FILE}" <<'EOF'

networks:
  default:
    name: ollama_network
EOF
    fi
}

download_recommended_models() {
    local model="${1:-phi3:3.8b}"

    log_info "TÃ©lÃ©chargement modÃ¨le ${model}..."
    log_info "Ceci peut prendre 5-10 min selon modÃ¨le..."

    if docker exec ollama ollama pull "${model}"; then
        log_success "ModÃ¨le ${model} tÃ©lÃ©chargÃ© !"
    else
        log_warn "Ã‰chec tÃ©lÃ©chargement. TÃ©lÃ©chargez manuellement :"
        log_warn "  docker exec ollama ollama pull ${model}"
    fi
}

update_homepage() {
    local homepage_config="${USER_HOME}/stacks/homepage/config/services.yaml"
    [[ ! -f "${homepage_config}" ]] && return

    if grep -q "Ollama Chat:" "${homepage_config}"; then
        return
    fi

    log_info "Ajout Ollama au dashboard Homepage..."
    cat >> "${homepage_config}" <<EOF

- Intelligence Artificielle:
    - Ollama Chat:
        href: http://raspberrypi.local:3000
        description: LLM Local (ChatGPT alternative)
        icon: https://ollama.com/public/ollama.png
EOF

    docker restart homepage >/dev/null 2>&1 || true
    log_success "Ollama ajoutÃ© au dashboard"
}

create_usage_guide() {
    cat > "${STACK_DIR}/USAGE.md" <<EOF
# ğŸ¤– Guide Ollama + Open WebUI

## AccÃ¨s

**Interface Web** : http://raspberrypi.local:3000

PremiÃ¨re connexion :
1. CrÃ©er compte admin
2. Choisir modÃ¨le dans menu dÃ©roulant
3. Commencer Ã  chatter !

---

## ğŸ“¥ TÃ©lÃ©charger ModÃ¨les

### Via Interface Web
Settings â†’ Models â†’ Pull Model

### Via CLI
\`\`\`bash
docker exec -it ollama ollama pull <model-name>
\`\`\`

### ModÃ¨les RecommandÃ©s Pi 5

**LÃ©gers (< 1GB)** :
- \`tinyllama:1.1b\` - Questions simples, ultra-rapide
- \`deepseek-coder:1.3b\` - Code seulement

**Ã‰quilibrÃ©s (2-4GB)** :
- \`phi3:3.8b\` â­ - Meilleur rapport qualitÃ©/vitesse
- \`qwen2.5-coder:3b\` - Code + raisonnement

**AvancÃ©s (7GB+)** - Lent sur Pi 5
- \`llama3:7b\` - GPT-3.5 quality
- \`mistral:7b\` - Excellent franÃ§ais

---

## ğŸ’¡ Exemples Prompts

### GÃ©nÃ©ration Code
\`\`\`
Ã‰cris une fonction Python qui calcule la suite de Fibonacci
\`\`\`

### RÃ©sumÃ© Documents
\`\`\`
RÃ©sume ce texte en 3 points clÃ©s : [coller texte]
\`\`\`

### Traduction
\`\`\`
Traduis en franÃ§ais : [texte anglais]
\`\`\`

### Debug Code
\`\`\`
Pourquoi ce code Python ne fonctionne pas ?
[coller code]
\`\`\`

---

## ğŸ”§ Commandes Utiles

### Lister modÃ¨les installÃ©s
\`\`\`bash
docker exec ollama ollama list
\`\`\`

### Supprimer modÃ¨le
\`\`\`bash
docker exec ollama ollama rm <model-name>
\`\`\`

### Tester modÃ¨le en CLI
\`\`\`bash
docker exec -it ollama ollama run phi3:3.8b
\`\`\`

### Voir logs
\`\`\`bash
docker-compose logs -f ollama
docker-compose logs -f open-webui
\`\`\`

---

## ğŸ“Š Performance Pi 5

| ModÃ¨le | Taille | Tokens/sec | RAM UtilisÃ©e |
|--------|--------|------------|--------------|
| tinyllama:1.1b | 600MB | ~8-10 | ~1GB |
| phi3:3.8b | 2.3GB | ~3-5 | ~3GB |
| deepseek-coder:1.3b | 800MB | ~6-8 | ~1.5GB |
| llama3:7b | 4GB | ~1-2 | ~6GB |

---

## ğŸ”— API Ollama

L'API Ollama est compatible OpenAI :

\`\`\`bash
curl http://localhost:11434/api/generate -d '{
  "model": "phi3:3.8b",
  "prompt": "Pourquoi le ciel est bleu ?"
}'
\`\`\`

**Utiliser dans code** :
\`\`\`python
import requests

response = requests.post('http://localhost:11434/api/generate', json={
    'model': 'phi3:3.8b',
    'prompt': 'Explique Docker en 2 phrases'
})
\`\`\`

---

## ğŸš€ IntÃ©grations

### Continue.dev (VSCode)
1. Installer extension Continue
2. Settings â†’ Ollama
3. URL : http://localhost:11434
4. ModÃ¨le : phi3:3.8b

### n8n (Automatisation)
1. Node Ollama disponible
2. URL : http://ollama:11434
3. CrÃ©er workflows IA

---

## âš ï¸ Troubleshooting

### ModÃ¨le ne rÃ©pond pas
- VÃ©rifier RAM disponible : \`free -h\`
- RedÃ©marrer : \`docker-compose restart ollama\`

### Lent / Freeze
- Utiliser modÃ¨le plus lÃ©ger
- VÃ©rifier tempÃ©rature CPU : \`vcgencmd measure_temp\`

### Erreur mÃ©moire
- Limiter taille contexte dans Settings
- Fermer autres applications

---

## ğŸ“š Ressources

- ModÃ¨les : https://ollama.com/library
- Documentation : https://github.com/ollama/ollama
- Open WebUI : https://docs.openwebui.com/
- Community : r/LocalLLaMA (Reddit)
EOF
}

#######################
# MAIN
#######################

main() {
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "  Ollama + Open WebUI - LLM Self-Hosted"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""

    # VÃ©rifier si dÃ©jÃ  installÃ© (idempotent)
    if docker ps --format '{{.Names}}' | grep -q "^ollama$"; then
        log_success "Ollama dÃ©jÃ  installÃ©"
        docker ps --filter "name=ollama" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
        echo ""
        echo "ğŸ¤– Interface : http://raspberrypi.local:3000"
        echo "ğŸ”§ API : http://raspberrypi.local:11434"
        return 0
    fi

    check_requirements

    mkdir -p "${STACK_DIR}" "${MODELS_DIR}"
    cd "${STACK_DIR}"

    detect_traefik_scenario
    create_env
    create_compose

    log_info "DÃ©ploiement Ollama + Open WebUI..."
    log_warn "Images Docker (~3 GB) - TÃ©lÃ©chargement en cours..."
    echo ""

    # Lancer en dÃ©tachÃ©
    docker compose up -d > /dev/null 2>&1 &

    log_success "DÃ©ploiement lancÃ© en arriÃ¨re-plan"
    log_info "Suivre progression :"
    log_info "  cd ${STACK_DIR} && docker compose logs -f"

    # IntÃ©grations
    update_homepage
    create_usage_guide

    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ‰ OLLAMA + OPEN WEBUI EN COURS D'INSTALLATION"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "â³ Les images Docker sont en cours de tÃ©lÃ©chargement (~3 GB)"
    echo "   Ceci peut prendre 10-20 min sur Raspberry Pi 5"
    echo ""
    echo "ğŸ“Š VÃ©rifier progression :"
    echo "   cd ${STACK_DIR}"
    echo "   docker compose logs -f"
    echo ""
    echo "ğŸ” VÃ©rifier status :"
    echo "   docker ps"
    echo ""
    echo "ğŸ¤– Une fois prÃªt :"
    echo "   Interface : http://raspberrypi.local:3000"
    [[ "${TRAEFIK_SCENARIO}" == "duckdns" ]] && echo "   Public : https://ai.${DUCKDNS_SUBDOMAIN}.duckdns.org"
    [[ "${TRAEFIK_SCENARIO}" == "cloudflare" ]] && echo "   Public : https://ai.${DOMAIN}"
    echo "   API : http://raspberrypi.local:11434"
    echo ""
    echo "ğŸ“‹ Guide : ${STACK_DIR}/USAGE.md"
    echo ""
    echo "ğŸ’¡ TÃ©lÃ©charger modÃ¨le (une fois Ollama UP) :"
    echo "   docker exec ollama ollama pull phi3:3.8b"
    echo "   Liste : https://ollama.com/library"
}

main "$@"
