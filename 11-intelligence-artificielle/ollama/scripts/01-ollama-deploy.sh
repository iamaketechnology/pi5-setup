#!/usr/bin/env bash
#
# Ollama + Open WebUI Deployment Script - Phase 21
# LLM Self-Hosted (ChatGPT alternative) avec interface Web
#
# Sources officielles :
# - Ollama: https://github.com/ollama/ollama
# - Open WebUI: https://github.com/open-webui/open-webui
#
# Ce script est IDEMPOTENT : peut √™tre ex√©cut√© plusieurs fois sans probl√®me

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../../.." && pwd)"
source "${PROJECT_ROOT}/common-scripts/lib.sh"

STACK_NAME="ollama"
STACK_DIR="${HOME}/stacks/${STACK_NAME}"
COMPOSE_FILE="${STACK_DIR}/docker-compose.yml"
ENV_FILE="${STACK_DIR}/.env"
MODELS_DIR="${HOME}/data/ollama/models"

TRAEFIK_ENV="${HOME}/stacks/traefik/.env"
TRAEFIK_SCENARIO="none"

#######################
# FONCTIONS
#######################

check_requirements() {
    log_info "V√©rification configuration requise..."

    # Check RAM (minimum 8GB recommand√©)
    local total_ram
    total_ram=$(free -g | awk '/^Mem:/{print $2}')

    if [[ ${total_ram} -lt 8 ]]; then
        log_warn "‚ö†Ô∏è  RAM d√©tect√©e : ${total_ram}GB"
        log_warn "   Recommand√© : 8GB minimum pour LLM"
        echo ""
        echo "Vous pouvez continuer mais les performances seront limit√©es."
        echo "Utilisez des mod√®les l√©gers (TinyLlama, Phi-2)."
        echo ""
        read -p "Continuer ? (y/N) " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 0
        fi
    else
        log_success "RAM : ${total_ram}GB ‚úì"
    fi

    # Check architecture
    local arch
    arch=$(uname -m)
    if [[ "${arch}" != "aarch64" ]] && [[ "${arch}" != "arm64" ]]; then
        log_error "Architecture non support√©e : ${arch}"
        log_warn "Ollama n√©cessite ARM64 (aarch64)"
        exit 1
    fi
    log_success "Architecture : ${arch} ‚úì"
}

detect_traefik_scenario() {
    [[ ! -f "${TRAEFIK_ENV}" ]] && return

    if grep -q "DUCKDNS_SUBDOMAIN" "${TRAEFIK_ENV}" 2>/dev/null; then
        TRAEFIK_SCENARIO="duckdns"
        DUCKDNS_SUBDOMAIN=$(grep "^DUCKDNS_SUBDOMAIN=" "${TRAEFIK_ENV}" | cut -d'=' -f2)
    elif grep -q "CLOUDFLARE_API_TOKEN" "${TRAEFIK_ENV}" 2>/dev/null; then
        TRAEFIK_SCENARIO="cloudflare"
        DOMAIN=$(grep "^DOMAIN=" "${TRAEFIK_ENV}" | cut -d'=' -f2)
    elif grep -q "VPN_MODE" "${TRAEFIK_ENV}" 2>/dev/null; then
        TRAEFIK_SCENARIO="vpn"
    fi
}

create_env() {
    cat > "${ENV_FILE}" <<EOF
# Ollama + Open WebUI Configuration
# G√©n√©r√© le $(date)

# Ollama
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_MODELS=${MODELS_DIR}

# Open WebUI
WEBUI_SECRET_KEY=$(openssl rand -hex 32)
WEBUI_NAME="Pi5 AI Chat"

# Timezone
TZ=Europe/Paris
EOF

    chmod 600 "${ENV_FILE}"
}

create_compose() {
    cat > "${COMPOSE_FILE}" <<'EOF'
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    volumes:
      - ${OLLAMA_MODELS}:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    volumes:
      - ./data:/app/backend/data
    environment:
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - WEBUI_NAME=${WEBUI_NAME}
      - OLLAMA_BASE_URL=http://ollama:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
EOF

    # Ajouter Traefik si d√©tect√©
    if [[ "${TRAEFIK_SCENARIO}" != "none" ]]; then
        cat >> "${COMPOSE_FILE}" <<'EOF'

networks:
  default:
    name: ollama-network
  traefik-network:
    external: true
EOF

        sed -i '' '/open-webui:/a\
    networks:\
      - default\
      - traefik-network\
    labels:\
      - "traefik.enable=true"\
      - "traefik.http.services.ollama-ui.loadbalancer.server.port=8080"
' "${COMPOSE_FILE}"

        case "${TRAEFIK_SCENARIO}" in
            duckdns)
                sed -i '' '/traefik.http.services.ollama-ui/a\
      - "traefik.http.routers.ollama-ui.rule=Host(`ai.'"${DUCKDNS_SUBDOMAIN}"'.duckdns.org`)"\
      - "traefik.http.routers.ollama-ui.entrypoints=websecure"\
      - "traefik.http.routers.ollama-ui.tls.certresolver=letsencrypt"
' "${COMPOSE_FILE}"
                ;;
            cloudflare)
                sed -i '' '/traefik.http.services.ollama-ui/a\
      - "traefik.http.routers.ollama-ui.rule=Host(`ai.'"${DOMAIN}"'`)"\
      - "traefik.http.routers.ollama-ui.entrypoints=websecure"\
      - "traefik.http.routers.ollama-ui.tls.certresolver=letsencrypt"
' "${COMPOSE_FILE}"
                ;;
        esac
    fi
}

download_recommended_models() {
    log_info "T√©l√©chargement mod√®les recommand√©s pour Pi 5..."

    echo ""
    echo "Mod√®les disponibles :"
    echo "  1. tinyllama:1.1b    (600MB)  - Ultra-rapide, questions simples"
    echo "  2. phi3:3.8b         (2.3GB)  - Meilleur √©quilibre qualit√©/vitesse ‚≠ê"
    echo "  3. deepseek-coder:1.3b (800MB) - Sp√©cialis√© code"
    echo "  4. Aucun (t√©l√©charger manuellement plus tard)"
    echo ""
    read -p "Choisir mod√®le √† t√©l√©charger (1-4) [2]: " choice
    choice=${choice:-2}

    local model
    case ${choice} in
        1) model="tinyllama:1.1b" ;;
        2) model="phi3:3.8b" ;;
        3) model="deepseek-coder:1.3b" ;;
        4)
            log_info "Aucun mod√®le t√©l√©charg√©"
            return
            ;;
        *)
            log_warn "Choix invalide, utilisation phi3:3.8b"
            model="phi3:3.8b"
            ;;
    esac

    log_info "T√©l√©chargement ${model} (ceci peut prendre 5-10 min)..."
    docker exec ollama ollama pull "${model}"
    log_success "Mod√®le ${model} t√©l√©charg√© !"
}

update_homepage() {
    local homepage_config="${HOME}/stacks/homepage/config/services.yaml"
    [[ ! -f "${homepage_config}" ]] && return

    if grep -q "Ollama:" "${homepage_config}"; then
        return
    fi

    cat >> "${homepage_config}" <<EOF

- Intelligence Artificielle:
    - Ollama Chat:
        href: http://raspberrypi.local:3000
        description: LLM Local (ChatGPT alternative)
        icon: https://ollama.com/public/ollama.png
EOF

    docker restart homepage >/dev/null 2>&1 || true
}

create_usage_guide() {
    cat > "${STACK_DIR}/USAGE.md" <<EOF
# ü§ñ Guide Ollama + Open WebUI

## Acc√®s

**Interface Web** : http://raspberrypi.local:3000

Premi√®re connexion :
1. Cr√©er compte admin
2. Choisir mod√®le dans menu d√©roulant
3. Commencer √† chatter !

---

## üì• T√©l√©charger Mod√®les

### Via Interface Web
Settings ‚Üí Models ‚Üí Pull Model

### Via CLI
\`\`\`bash
docker exec -it ollama ollama pull <model-name>
\`\`\`

### Mod√®les Recommand√©s Pi 5

**L√©gers (< 1GB)** :
- \`tinyllama:1.1b\` - Questions simples, ultra-rapide
- \`deepseek-coder:1.3b\` - Code seulement

**√âquilibr√©s (2-4GB)** :
- \`phi3:3.8b\` ‚≠ê - Meilleur rapport qualit√©/vitesse
- \`qwen2.5-coder:3b\` - Code + raisonnement

**Avanc√©s (7GB+)** - Lent sur Pi 5
- \`llama3:7b\` - GPT-3.5 quality
- \`mistral:7b\` - Excellent fran√ßais

---

## üí° Exemples Prompts

### G√©n√©ration Code
\`\`\`
√âcris une fonction Python qui calcule la suite de Fibonacci
\`\`\`

### R√©sum√© Documents
\`\`\`
R√©sume ce texte en 3 points cl√©s : [coller texte]
\`\`\`

### Traduction
\`\`\`
Traduis en fran√ßais : [texte anglais]
\`\`\`

### Debug Code
\`\`\`
Pourquoi ce code Python ne fonctionne pas ?
[coller code]
\`\`\`

---

## üîß Commandes Utiles

### Lister mod√®les install√©s
\`\`\`bash
docker exec ollama ollama list
\`\`\`

### Supprimer mod√®le
\`\`\`bash
docker exec ollama ollama rm <model-name>
\`\`\`

### Tester mod√®le en CLI
\`\`\`bash
docker exec -it ollama ollama run phi3:3.8b
\`\`\`

### Voir logs
\`\`\`bash
docker-compose logs -f ollama
docker-compose logs -f open-webui
\`\`\`

---

## üìä Performance Pi 5

| Mod√®le | Taille | Tokens/sec | RAM Utilis√©e |
|--------|--------|------------|--------------|
| tinyllama:1.1b | 600MB | ~8-10 | ~1GB |
| phi3:3.8b | 2.3GB | ~3-5 | ~3GB |
| deepseek-coder:1.3b | 800MB | ~6-8 | ~1.5GB |
| llama3:7b | 4GB | ~1-2 | ~6GB |

---

## üîó API Ollama

L'API Ollama est compatible OpenAI :

\`\`\`bash
curl http://localhost:11434/api/generate -d '{
  "model": "phi3:3.8b",
  "prompt": "Pourquoi le ciel est bleu ?"
}'
\`\`\`

**Utiliser dans code** :
\`\`\`python
import requests

response = requests.post('http://localhost:11434/api/generate', json={
    'model': 'phi3:3.8b',
    'prompt': 'Explique Docker en 2 phrases'
})
\`\`\`

---

## üöÄ Int√©grations

### Continue.dev (VSCode)
1. Installer extension Continue
2. Settings ‚Üí Ollama
3. URL : http://localhost:11434
4. Mod√®le : phi3:3.8b

### n8n (Automatisation)
1. Node Ollama disponible
2. URL : http://ollama:11434
3. Cr√©er workflows IA

---

## ‚ö†Ô∏è Troubleshooting

### Mod√®le ne r√©pond pas
- V√©rifier RAM disponible : \`free -h\`
- Red√©marrer : \`docker-compose restart ollama\`

### Lent / Freeze
- Utiliser mod√®le plus l√©ger
- V√©rifier temp√©rature CPU : \`vcgencmd measure_temp\`

### Erreur m√©moire
- Limiter taille contexte dans Settings
- Fermer autres applications

---

## üìö Ressources

- Mod√®les : https://ollama.com/library
- Documentation : https://github.com/ollama/ollama
- Open WebUI : https://docs.openwebui.com/
- Community : r/LocalLLaMA (Reddit)
EOF
}

#######################
# MAIN
#######################

main() {
    print_header "Ollama + Open WebUI - LLM Self-Hosted"

    log_info "Installation Phase 21 - Intelligence Artificielle..."
    echo ""

    check_requirements

    mkdir -p "${STACK_DIR}" "${MODELS_DIR}"
    cd "${STACK_DIR}"

    detect_traefik_scenario
    create_env
    create_compose

    log_info "D√©ploiement Ollama + Open WebUI..."
    docker-compose up -d

    log_info "Attente d√©marrage services (60s)..."
    sleep 60

    if docker ps | grep -q "ollama"; then
        log_success "Ollama d√©marr√© !"
    else
        log_error "√âchec d√©marrage Ollama"
        docker-compose logs
        exit 1
    fi

    # T√©l√©charger mod√®le
    download_recommended_models

    # Int√©grations
    update_homepage
    create_usage_guide

    echo ""
    print_section "Ollama + Open WebUI Install√© !"
    echo ""
    echo "ü§ñ Acc√®s Interface :"
    echo "   http://raspberrypi.local:3000"
    echo ""
    echo "üîß API Ollama :"
    echo "   http://raspberrypi.local:11434"
    echo ""
    echo "üìã Guide complet :"
    echo "   cat ${STACK_DIR}/USAGE.md"
    echo ""
    echo "üìä Ressources :"
    echo "   RAM : ~2-4 GB (selon mod√®le charg√©)"
    echo "   Mod√®les : ${MODELS_DIR}"
    echo ""
    echo "üí° Prochaines √©tapes :"
    echo "   1. Ouvrir http://raspberrypi.local:3000"
    echo "   2. Cr√©er compte admin"
    echo "   3. S√©lectionner mod√®le et commencer √† chatter !"
    echo ""
    echo "üîß T√©l√©charger plus de mod√®les :"
    echo "   docker exec ollama ollama pull <model-name>"
    echo "   Liste : https://ollama.com/library"
    echo ""

    log_success "Installation termin√©e !"
}

main "$@"
