#!/bin/bash
# =============================================================================
# Ollama Models Downloader - Raspberry Pi 5 Optimized
# =============================================================================
# Version: 1.0.0
# Last updated: 2025-01-14
# Author: PI5-SETUP Project
# Usage: sudo bash 02-download-models.sh
# =============================================================================
# TÃ©lÃ©charge modÃ¨les LLM optimisÃ©s pour Pi5 16GB par catÃ©gorie d'usage
# BasÃ© sur benchmarks 2025 (tokens/sec, RAM, qualitÃ©)
# =============================================================================

set -euo pipefail

# === Logging functions ===
log_info() { echo -e "\033[0;34m[INFO]\033[0m $*"; }
log_error() { echo -e "\033[0;31m[ERROR]\033[0m $*" >&2; }
log_success() { echo -e "\033[0;32m[SUCCESS]\033[0m $*"; }
log_warn() { echo -e "\033[0;33m[WARN]\033[0m $*"; }

# === Check Ollama running ===
if ! docker ps --format '{{.Names}}' | grep -q "^ollama$"; then
    log_error "Ollama n'est pas dÃ©marrÃ©"
    log_info "DÃ©marrez-le avec : cd ~/stacks/ollama && docker compose up -d"
    exit 1
fi

log_success "Ollama dÃ©tectÃ©"

# === ModÃ¨les disponibles (optimisÃ©s Pi5 16GB) ===
declare -A MODELS

# CatÃ©gorie: Usage GÃ©nÃ©ral (rapides, polyvalents)
MODELS[1_name]="gemma2:2b"
MODELS[1_cat]="âš¡ Ultra-Rapide"
MODELS[1_desc]="Google Gemma 2B - Excellent rapport vitesse/qualitÃ© (8-10 tok/s)"
MODELS[1_ram]="~2 GB"
MODELS[1_size]="1.6 GB"

MODELS[2_name]="phi3:3.8b"
MODELS[2_cat]="ğŸ¯ Ã‰quilibrÃ©"
MODELS[2_desc]="Microsoft Phi3 - Meilleur Ã©quilibre perf/qualitÃ© (3-5 tok/s)"
MODELS[2_ram]="~3 GB"
MODELS[2_size]="2.3 GB"

MODELS[3_name]="llama3.2:3b"
MODELS[3_cat]="ğŸ¯ Ã‰quilibrÃ©"
MODELS[3_desc]="Meta Llama 3.2 - Excellent pour conversations (3-4 tok/s)"
MODELS[3_ram]="~3 GB"
MODELS[3_size]="2.0 GB"

# CatÃ©gorie: Code & DevOps
MODELS[4_name]="qwen2.5-coder:1.5b"
MODELS[4_cat]="ğŸ’» Code"
MODELS[4_desc]="Alibaba Qwen Coder - SpÃ©cialisÃ© code (rapide, 6-8 tok/s)"
MODELS[4_ram]="~2 GB"
MODELS[4_size]="984 MB"

MODELS[5_name]="qwen2.5-coder:3b"
MODELS[5_cat]="ğŸ’» Code"
MODELS[5_desc]="Qwen Coder 3B - Code + raisonnement (4-5 tok/s)"
MODELS[5_ram]="~3 GB"
MODELS[5_size]="1.9 GB"

MODELS[6_name]="deepseek-coder:1.3b"
MODELS[6_cat]="ğŸ’» Code"
MODELS[6_desc]="DeepSeek Coder - Ultra-rapide pour snippets (7-9 tok/s)"
MODELS[6_ram]="~1.5 GB"
MODELS[6_size]="776 MB"

# CatÃ©gorie: Multilingue & Maths
MODELS[7_name]="qwen2.5:3b"
MODELS[7_cat]="ğŸŒ Multilingue"
MODELS[7_desc]="Qwen 2.5 - 29 langues, excellent maths (4-5 tok/s)"
MODELS[7_ram]="~3 GB"
MODELS[7_size]="1.9 GB"

MODELS[8_name]="qwen2.5:7b"
MODELS[8_cat]="ğŸŒ Multilingue"
MODELS[8_desc]="Qwen 2.5 7B - Raisonnement avancÃ© (2-3 tok/s)"
MODELS[8_ram]="~6 GB"
MODELS[8_size]="4.7 GB"

# CatÃ©gorie: Haute QualitÃ© (lent mais prÃ©cis)
MODELS[9_name]="mistral:7b"
MODELS[9_cat]="ğŸ† QualitÃ©"
MODELS[9_desc]="Mistral 7B - Excellent franÃ§ais, moins rÃ©pÃ©titif (1-2 tok/s)"
MODELS[9_ram]="~6 GB"
MODELS[9_size]="4.1 GB"

MODELS[10_name]="llama3:8b"
MODELS[10_cat]="ğŸ† QualitÃ©"
MODELS[10_desc]="Meta Llama 3 8B - QualitÃ© GPT-3.5 (1-2 tok/s)"
MODELS[10_ram]="~7 GB"
MODELS[10_size]="4.7 GB"

# CatÃ©gorie: Vision (multimodal)
MODELS[11_name]="llava:7b"
MODELS[11_cat]="ğŸ‘ï¸ Vision"
MODELS[11_desc]="LLaVA - Analyse d'images (lent, ~1 tok/s)"
MODELS[11_ram]="~8 GB"
MODELS[11_size]="4.7 GB"

# === Afficher menu ===
show_menu() {
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "  ğŸ“¦ ModÃ¨les LLM OptimisÃ©s Raspberry Pi 5 (16 GB)"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "CatÃ©gories disponibles :"
    echo ""
    echo "  [1-3]   âš¡ Usage GÃ©nÃ©ral (rapides, polyvalents)"
    echo "  [4-6]   ğŸ’» Code & DevOps"
    echo "  [7-8]   ğŸŒ Multilingue & Maths"
    echo "  [9-10]  ğŸ† Haute QualitÃ© (lent mais prÃ©cis)"
    echo "  [11]    ğŸ‘ï¸ Vision (analyse d'images)"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""

    for i in {1..11}; do
        local name="${MODELS[${i}_name]}"
        local cat="${MODELS[${i}_cat]}"
        local desc="${MODELS[${i}_desc]}"
        local ram="${MODELS[${i}_ram]}"
        local size="${MODELS[${i}_size]}"

        printf "  [%2d] %s\n" "$i" "$cat"
        printf "       ğŸ“¦ %-25s | RAM: %-7s | Size: %s\n" "$name" "$ram" "$size"
        printf "       %s\n\n" "$desc"
    done

    echo "  [12] ğŸ Pack RecommandÃ© (gemma2:2b + phi3:3.8b + qwen2.5-coder:1.5b)"
    echo "  [13] ğŸš€ Pack Performance (gemma2:2b + llama3.2:3b + deepseek-coder:1.3b)"
    echo "  [14] ğŸ’ª Pack Complet (tous les modÃ¨les < 4GB)"
    echo ""
    echo "  [0]  âŒ Quitter"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
}

# === Liste modÃ¨les dÃ©jÃ  installÃ©s ===
list_installed() {
    echo ""
    log_info "ModÃ¨les dÃ©jÃ  installÃ©s :"
    docker exec ollama ollama list 2>/dev/null || echo "Aucun"
}

# === TÃ©lÃ©charger modÃ¨le ===
download_model() {
    local model=$1
    local name=$(echo "$model" | cut -d':' -f1)

    log_info "VÃ©rification si $model existe dÃ©jÃ ..."

    # Check si dÃ©jÃ  installÃ© (idempotent)
    if docker exec ollama ollama list 2>/dev/null | grep -q "^${model}"; then
        log_success "$model dÃ©jÃ  installÃ© (skip)"
        return 0
    fi

    log_info "TÃ©lÃ©chargement $model..."
    log_warn "Ceci peut prendre 2-10 min selon la taille"
    echo ""

    if docker exec ollama ollama pull "$model"; then
        log_success "$model tÃ©lÃ©chargÃ© !"
    else
        log_error "Ã‰chec tÃ©lÃ©chargement $model"
        return 1
    fi
}

# === TÃ©lÃ©charger pack ===
download_pack() {
    local pack_name=$1
    shift
    local models=("$@")

    log_info "TÃ©lÃ©chargement $pack_name..."
    echo ""

    for model in "${models[@]}"; do
        download_model "$model"
    done

    log_success "Pack $pack_name installÃ© !"
}

# === Main ===
main() {
    list_installed

    while true; do
        show_menu

        read -p "SÃ©lectionnez un modÃ¨le [0-14] : " choice

        case $choice in
            0)
                log_info "Au revoir !"
                exit 0
                ;;
            [1-9]|1[0-1])
                local model_name="${MODELS[${choice}_name]}"
                download_model "$model_name"
                list_installed
                ;;
            12)
                download_pack "Pack RecommandÃ©" "gemma2:2b" "phi3:3.8b" "qwen2.5-coder:1.5b"
                list_installed
                ;;
            13)
                download_pack "Pack Performance" "gemma2:2b" "llama3.2:3b" "deepseek-coder:1.3b"
                list_installed
                ;;
            14)
                download_pack "Pack Complet" "gemma2:2b" "phi3:3.8b" "llama3.2:3b" \
                    "qwen2.5-coder:1.5b" "qwen2.5-coder:3b" "deepseek-coder:1.3b" "qwen2.5:3b"
                list_installed
                ;;
            *)
                log_error "Choix invalide"
                ;;
        esac

        echo ""
        read -p "TÃ©lÃ©charger un autre modÃ¨le ? (y/N) " again
        [[ ! "$again" =~ ^[Yy]$ ]] && break
    done

    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "âœ… TÃ‰LÃ‰CHARGEMENT TERMINÃ‰"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    log_info "AccÃ©dez Ã  Open WebUI : http://pi5.local:3002"
    log_info "SÃ©lectionnez un modÃ¨le dans le menu dÃ©roulant"
}

main "$@"
