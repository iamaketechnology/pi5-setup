# ü§ñ Ollama + Open WebUI - LLM Local sur Raspberry Pi 5

> **H√©bergez vos propres mod√®les de langage (LLM) et une interface de chat de type ChatGPT, 100% en local sur votre Pi.**

[![Version](https://img.shields.io/badge/version-1.0-blue.svg)](CHANGELOG.md)
[![Pi 5](https://img.shields.io/badge/Raspberry%20Pi-5-red.svg)](https://www.raspberrypi.com/)
[![Ollama](https://img.shields.io/badge/Ollama-Latest-green.svg)](https://ollama.ai/)

---

## üéØ Vue d'Ensemble

Ce stack d√©ploie **Ollama**, un serveur de mod√®les de langage (LLM) l√©ger et puissant, ainsi que **Open WebUI**, une interface de chat conviviale et r√©active. Ensemble, ils offrent une exp√©rience similaire √† ChatGPT, mais enti√®rement auto-h√©berg√©e, priv√©e et sans frais.

### ü§î Pourquoi h√©berger son propre LLM ?

- **Confidentialit√© Totale** : Vos conversations avec l'IA ne quittent jamais votre Raspberry Pi. Aucune donn√©e n'est envoy√©e √† OpenAI, Google ou toute autre entreprise.
- **Pas de Censure** : Utilisez des mod√®les non censur√©s pour des r√©ponses plus directes et cr√©atives.
- **Gratuit et Illimit√©** : Pas de frais par requ√™te, pas de limite d'utilisation. La seule limite est la puissance de votre Pi.
- **Personnalisation** : Acc√©dez √† des centaines de mod√®les open-source (Llama 3, Mistral, Phi-3, etc.) et cr√©ez des versions personnalis√©es.
- **Acc√®s API** : Utilisez l'API compatible OpenAI d'Ollama pour int√©grer l'IA dans vos propres applications et scripts.

---

## üèóÔ∏è Architecture

Le stack est compos√© de deux services principaux :

```
ollama/
‚îú‚îÄ‚îÄ ollama         # Le serveur principal qui fait tourner les LLMs
‚îî‚îÄ‚îÄ open-webui     # L'interface de chat web qui communique avec Ollama
```

- **Ollama** √©coute sur le port `11434` et expose une API pour charger, d√©charger et interroger les mod√®les.
- **Open WebUI** √©coute sur le port `3002` (ajust√© pour √©viter conflit avec Supabase Studio) et fournit l'interface utilisateur.

---

## üöÄ Installation

L'installation est enti√®rement automatis√©e via un script shell.

```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/01-ollama-deploy.sh | sudo bash
```

Pour des instructions d√©taill√©es, consultez le guide d'installation :
- **[üìÑ ollama-setup.md](ollama-setup.md)**

---

## ‚öôÔ∏è Configuration

### Mod√®les de Langage

Le script d'installation configure Ollama mais **ne t√©l√©charge pas automatiquement de mod√®le**. Utilisez le script de t√©l√©chargement intelligent pour obtenir les meilleurs mod√®les pour votre Pi 5.

#### üéØ T√©l√©chargement Intelligent (Recommand√©)

**Script interactif avec 11 mod√®les optimis√©s pour Pi 5 16GB** :

```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/02-download-models.sh | sudo bash
```

**Fonctionnalit√©s** :
- ‚úÖ Menu interactif avec descriptions et benchmarks
- ‚úÖ 3 packs pr√©-configur√©s (Recommand√©, D√©veloppeur, Multilingue)
- ‚úÖ S√©lection multiple de mod√®les
- ‚úÖ Idempotent (skip si d√©j√† install√©)
- ‚úÖ Mod√®les optimis√©s pour performances Pi 5 (8-10 tok/s)

**Mod√®les disponibles** :
- **Chat rapide** : gemma2:2b (8-10 tok/s), llama3.2:3b
- **Code** : qwen2.5-coder:1.5b, deepseek-coder-v2:16b
- **Multilingue** : aya-expanse:8b (100+ langues)
- **Vision** : llava:7b (analyse d'images)
- **Multit√¢che** : phi3:3.8b, mistral:7b

#### üîÑ Mise √† Jour Automatique

**Script de mise √† jour avec support cron** :

```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/03-update-models.sh | sudo bash
```

**Planifier les mises √† jour hebdomadaires** :
```bash
# Ex√©cuter le script avec --setup-cron
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/03-update-models.sh | sudo bash -s -- --setup-cron
```

#### üõ†Ô∏è Gestion Manuelle

Pour g√©rer les mod√®les manuellement :

```bash
# Lister les mod√®les install√©s
docker exec ollama ollama list

# T√©l√©charger un nouveau mod√®le (ex: Llama 3)
docker exec ollama ollama pull llama3

# Supprimer un mod√®le
docker exec ollama ollama rm phi3:3.8b
```

Une liste compl√®te des mod√®les disponibles se trouve sur la [biblioth√®que Ollama](https://ollama.com/library).

### Acc√®s √† l'API

L'API d'Ollama est accessible sur le port `11434` et est compatible avec l'API d'OpenAI, ce qui facilite l'int√©gration avec des outils existants.

```bash
curl http://localhost:11434/api/generate -d '{ "model": "phi3:3.8b", "prompt": "Pourquoi le ciel est-il bleu ?" }'
```

---

## üéì Pour Commencer

Pour apprendre √† utiliser l'interface, √† g√©rer les mod√®les et √† cr√©er des workflows IA, consultez notre guide pour d√©butants :
- **[üéì ollama-guide.md](ollama-guide.md)**
