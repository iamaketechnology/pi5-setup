# ü§ñ Ollama + Open WebUI - LLM Local sur Raspberry Pi 5

> **H√©bergez vos propres mod√®les de langage (LLM) et une interface de chat de type ChatGPT, 100% en local sur votre Pi.**

[![Version](https://img.shields.io/badge/version-1.0-blue.svg)](CHANGELOG.md)
[![Pi 5](https://img.shields.io/badge/Raspberry%20Pi-5-red.svg)](https://www.raspberrypi.com/)
[![Ollama](https://img.shields.io/badge/Ollama-Latest-green.svg)](https://ollama.ai/)

---

## üéØ Vue d'Ensemble

Ce stack d√©ploie **Ollama**, un serveur de mod√®les de langage (LLM) l√©ger et puissant, ainsi que **Open WebUI**, une interface de chat conviviale et r√©active. Ensemble, ils offrent une exp√©rience similaire √† ChatGPT, mais enti√®rement auto-h√©berg√©e, priv√©e et sans frais.

### ü§î Pourquoi h√©berger son propre LLM ?

- **Confidentialit√© Totale** : Vos conversations avec l'IA ne quittent jamais votre Raspberry Pi. Aucune donn√©e n'est envoy√©e √† OpenAI, Google ou toute autre entreprise.
- **Pas de Censure** : Utilisez des mod√®les non censur√©s pour des r√©ponses plus directes et cr√©atives.
- **Gratuit et Illimit√©** : Pas de frais par requ√™te, pas de limite d'utilisation. La seule limite est la puissance de votre Pi.
- **Personnalisation** : Acc√©dez √† des centaines de mod√®les open-source (Llama 3, Mistral, Phi-3, etc.) et cr√©ez des versions personnalis√©es.
- **Acc√®s API** : Utilisez l'API compatible OpenAI d'Ollama pour int√©grer l'IA dans vos propres applications et scripts.

---

## üèóÔ∏è Architecture

Le stack est compos√© de deux services principaux :

```
ollama/
‚îú‚îÄ‚îÄ ollama         # Le serveur principal qui fait tourner les LLMs
‚îî‚îÄ‚îÄ open-webui     # L'interface de chat web qui communique avec Ollama
```

- **Ollama** √©coute sur le port `11434` et expose une API pour charger, d√©charger et interroger les mod√®les.
- **Open WebUI** √©coute sur le port `8080` et fournit l'interface utilisateur, en appelant l'API d'Ollama en arri√®re-plan.

---

## üöÄ Installation

L'installation est enti√®rement automatis√©e via un script shell.

```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/01-ollama-deploy.sh | sudo bash
```

Pour des instructions d√©taill√©es, consultez le guide d'installation :
- **[üìÑ ollama-setup.md](ollama-setup.md)**

---

## ‚öôÔ∏è Configuration

### Mod√®les de Langage

Par d√©faut, le script d'installation t√©l√©charge le mod√®le `phi3:3.8b`, un excellent mod√®le l√©ger et performant.

Pour ajouter d'autres mod√®les, utilisez la ligne de commande :

```bash
# Lister les mod√®les install√©s
docker exec ollama ollama list

# T√©l√©charger un nouveau mod√®le (ex: Llama 3)
docker exec ollama ollama pull llama3

# Supprimer un mod√®le
docker exec ollama ollama rm phi3:3.8b
```

Une liste compl√®te des mod√®les disponibles se trouve sur la [biblioth√®que Ollama](https://ollama.com/library).

### Acc√®s √† l'API

L'API d'Ollama est accessible sur le port `11434` et est compatible avec l'API d'OpenAI, ce qui facilite l'int√©gration avec des outils existants.

```bash
curl http://localhost:11434/api/generate -d '{ "model": "phi3:3.8b", "prompt": "Pourquoi le ciel est-il bleu ?" }'
```

---

## üéì Pour Commencer

Pour apprendre √† utiliser l'interface, √† g√©rer les mod√®les et √† cr√©er des workflows IA, consultez notre guide pour d√©butants :
- **[üéì ollama-guide.md](ollama-guide.md)**
