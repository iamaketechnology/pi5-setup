# ‚ö° Installation Rapide - Ollama + Open WebUI

> **Installation directe via SSH pour h√©berger votre propre IA de chat.**

---

## üöÄ Installation en 1 Commande

**Copier-coller cette commande dans votre terminal SSH :**

```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/01-ollama-deploy.sh | sudo bash
```

**Ce qui sera d√©ploy√© :**
- ‚úÖ **Ollama** : Le serveur qui fait tourner les mod√®les de langage.
- ‚úÖ **Open WebUI** : L'interface de chat web, similaire √† ChatGPT.

**Dur√©e :**
- Images Docker : ~10-15 min (t√©l√©chargement en arri√®re-plan)
- Mod√®le IA (manuel) : ~5-10 min suppl√©mentaires

**Note** : Le script lance l'installation en arri√®re-plan et termine imm√©diatement. Suivez la progression avec `docker compose logs -f`.

---

## ‚úÖ V√©rification de l'Installation

### V√©rifier les services

```bash
cd ~/stacks/ollama
docker compose ps
```
Les services `ollama` et `open-webui` doivent √™tre en √©tat `Up` ou `Up (healthy)`.

### Acc√©der √† l'Interface Web

L'URL d√©pend de votre configuration Traefik :
- **Avec Traefik (DuckDNS)** : `https://ai.VOTRE-SOUS-DOMAINE.duckdns.org`
- **Avec Traefik (Cloudflare)** : `https://ai.VOTRE-DOMAINE.com`
- **Sans Traefik (local)** : `http://pi5.local:3002` ou `http://<IP-DU-PI>:3002`

‚ö†Ô∏è **Note** : Le port par d√©faut est **3002** (pas 3000) pour √©viter les conflits avec Supabase Studio.

L'URL exacte est affich√©e √† la fin du script d'installation.

---

## üë§ Configuration Initiale

1.  **V√©rifiez que les services sont UP** :
    ```bash
    docker ps | grep ollama
    ```
    Attendez que les deux containers (`ollama` et `open-webui`) soient en √©tat `Up`.

2.  **T√©l√©chargez vos premiers mod√®les** :

    **Option A : Script Intelligent (Recommand√©)**
    ```bash
    curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/02-download-models.sh | sudo bash
    ```
    - Menu interactif avec 11 mod√®les optimis√©s Pi 5
    - S√©lection multiple ou packs pr√©-configur√©s
    - Pack Recommand√© : gemma2:2b + phi3:3.8b + qwen2.5-coder:1.5b

    **Option B : Manuel (un seul mod√®le)**
    ```bash
    docker exec ollama ollama pull phi3:3.8b
    ```
    - T√©l√©charge phi3 uniquement (2.3 GB, ~5-10 min)

3.  **Ouvrez l'interface Open WebUI** avec l'URL ci-dessus.

4.  **Cr√©ez un compte administrateur** : La premi√®re fois, il vous sera demand√© de cr√©er un compte admin. Ce compte est pour l'interface web seulement.

5.  **S√©lectionnez le mod√®le** : En haut de l'interface, cliquez sur le menu d√©roulant et s√©lectionnez un mod√®le (ex: `gemma2:2b`).

6.  **Commencez √† chatter !**

---

## üß† G√©rer les Mod√®les de Langage

### üéØ T√©l√©chargement Intelligent (Recommand√©)

**Menu interactif avec mod√®les optimis√©s** :
```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/02-download-models.sh | sudo bash
```

**Mod√®les disponibles** :
- **Chat rapide** : gemma2:2b (8-10 tok/s), llama3.2:3b
- **Code** : qwen2.5-coder:1.5b, deepseek-coder-v2:16b
- **Multilingue** : aya-expanse:8b (100+ langues)
- **Vision** : llava:7b (analyse d'images)
- **Multit√¢che** : phi3:3.8b, mistral:7b

**Packs pr√©-configur√©s** :
- **Pack Recommand√©** : gemma2:2b + phi3:3.8b + qwen2.5-coder:1.5b
- **Pack D√©veloppeur** : qwen2.5-coder:1.5b + deepseek-coder-v2:16b + phi3:3.8b
- **Pack Multilingue** : aya-expanse:8b + gemma2:2b

### üîÑ Mise √† Jour Automatique

**Mettre √† jour tous les mod√®les install√©s** :
```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/03-update-models.sh | sudo bash
```

**Planifier les mises √† jour hebdomadaires** :
```bash
curl -fsSL https://raw.githubusercontent.com/iamaketechnology/pi5-setup/main/11-intelligence-artificielle/ollama/scripts/03-update-models.sh | sudo bash -s -- --setup-cron
```

### üõ†Ô∏è Gestion Manuelle

**T√©l√©charger un mod√®le sp√©cifique** :
1. Trouvez un mod√®le sur la [biblioth√®que Ollama](https://ollama.com/library) (ex: `llama3:8b`)
2. Ex√©cutez :
```bash
docker exec ollama ollama pull llama3:8b
```

**Lister les mod√®les install√©s** :
```bash
docker exec ollama ollama list
```

**Supprimer un mod√®le** :
```bash
docker exec ollama ollama rm phi3:3.8b
```

### üé® Changer de Mod√®le dans l'Interface Web

En haut de l'interface de chat, cliquez sur le nom du mod√®le actuel pour en s√©lectionner un autre parmi ceux que vous avez t√©l√©charg√©s.

---

## üìö Documentation Compl√®te

- **[ollama-guide.md](ollama-guide.md)** - Pour apprendre √† utiliser l'interface et √† choisir les bons mod√®les.
- **[README.md](README.md)** - Vue d'ensemble technique du stack Ollama.
