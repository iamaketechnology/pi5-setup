#!/bin/bash
# =============================================================================
# Script 3 : DÃ©ploiement Supabase Self-Hosted sur Raspberry Pi 5 (ARM64, 16GB RAM) - Version CorrigÃ©e
# Auteur : IngÃ©nieur DevOps ARM64 - OptimisÃ© pour Bookworm 64-bit (Kernel 6.12+)
# Version : 2.6.24-postgres-nopass-fix (Corrections: CREATE SCHEMA + GRANT via postgres sans mot de passe; GOTRUE_API_PORT=9999; DB_SCHEMA=realtime + search_path Ã©tendu)
# Objectif : Installer Supabase via Docker Compose avec configuration Kong complÃ¨te et migrations via images officielles.
# PrÃ©-requis : Script 1 (PrÃ©paration systÃ¨me, UFW) et Script 2 (Docker). Ports : 3000 (Studio), 8001 (API), 8082 (Meta).
# Usage : sudo SUPABASE_PORT=8001 ./setup-week2-supabase-finalG.sh
# Actions Post-Script : AccÃ©der http://IP:3000, crÃ©er un projet, noter les API keys (ANON_KEY, SERVICE_ROLE_KEY).
# Corrections v2.6.21 basÃ©es sur logs (21/09/2025):
# - Realtime: DROP SCHEMA realtime CASCADE avant CREATE/OWNER pour fix permission denied (owner postgres); DB_SCHEMA=realtime; search_path public,private,realtime.
# - Auth: Ajout GOTRUE_API_PORT=9999 pour listen correct (fix connection reset); healthcheck tolÃ©rÃ© pendant boot.
# - DÃ©pendances: service_started relaxÃ©; sleep 120s pour Kong/migrs; relance realtime post-up si exited.
# - ARM64: Buffers 128MB; ulimits 262144; tags officiels; no manual seeds.
# - Recherche: Docs Supabase GitHub (GOTRUE_API_PORT=9999; DB_SCHEMA=realtime; search_path incl. public/private).
# =============================================================================
set -euo pipefail  # ArrÃªt sur erreur, undefined vars, pipefail

# Fonctions de logging colorÃ©es pour traÃ§abilitÃ©
log()  { echo -e "\033[1;36m[SUPABASE]\033[0m $*"; }
warn() { echo -e "\033[1;33m[WARN]\033[0m $*"; }
ok()   { echo -e "\033[1;32m[OK]\033[0m $*"; }
error() { echo -e "\033[1;31m[ERROR]\033[0m $*"; exit 1; }

# Variables globales configurables
SCRIPT_VERSION="2.6.24-postgres-nopass-fix"
LOG_FILE="/var/log/supabase-setup-${SCRIPT_VERSION}-$(date +%Y%m%d_%H%M%S).log"
TARGET_USER="${SUDO_USER:-pi}"
PROJECT_DIR="/home/${TARGET_USER}/stacks/supabase"
SUPABASE_PORT="${SUPABASE_PORT:-8001}"
FORCE_RECREATE="${FORCE_RECREATE:-0}"
APP_NAME="realtime"  # Fix boot Elixir realtime
REALTIME_VERSION="v2.37.5"  # Tag stable rÃ©cent ARM64

# Redirection des logs vers fichier pour audit (stdout + fichier)
exec 1> >(tee -a "$LOG_FILE")
exec 2> >(tee -a "$LOG_FILE" >&2)
log "=== DÃ©but Supabase Setup v$SCRIPT_VERSION - $(date) ==="
log "Projet: $PROJECT_DIR | Port API: $SUPABASE_PORT | User: $TARGET_USER | Force recreate: $FORCE_RECREATE | APP_NAME: $APP_NAME"

# VÃ©rification exÃ©cution en root (nÃ©cessaire pour chown et Docker)
require_root() {
  if [[ $EUID -ne 0 ]]; then
    error "Ce script doit Ãªtre lancÃ© avec sudo: sudo SUPABASE_PORT=8001 $0"
  fi
}

# VÃ©rification des prÃ©-requis (Docker, page size, entropie) + auto-fix haveged
check_prereqs() {
  log "ðŸ” VÃ©rification prÃ©-requis..."
  if ! command -v docker &> /dev/null; then
    error "Docker manquant - ExÃ©cutez d'abord le Script 2 (Installation Docker)."
  fi
  if ! docker compose version | grep -q "2\."; then
    error "Docker Compose v2+ requis - VÃ©rifiez l'installation du Script 2."
  fi
  if ! getconf PAGESIZE | grep -q 4096; then
    error "Page size 4KB requis (reboot aprÃ¨s Script 1 si nÃ©cessaire)."
  fi
  local entropy=$(cat /proc/sys/kernel/random/entropy_avail 2>/dev/null || echo 0)
  if [[ $entropy -lt 256 ]]; then
    log "Entropie faible ($entropy) - Installation auto haveged pour fix."
    apt update && apt install -y haveged && systemctl enable --now haveged
    sleep 2  # Attente init
  fi
  if ! docker info 2>/dev/null | grep -q "systemd"; then
    warn "Cgroup driver non-systemd dÃ©tectÃ© - Warnings Docker possibles sur Pi5."
  fi
  ok "PrÃ©-requis validÃ©s avec succÃ¨s."
}

# Activation et configuration UFW (firewall) pour ports Supabase
activate_ufw() {
  log "ðŸ”¥ Activation et configuration UFW..."
  if ! ufw status | grep -q "Status: active"; then
    ufw --force enable
    log "UFW activÃ© avec succÃ¨s."
  else
    log "UFW dÃ©jÃ  actif."
  fi
  local ports=(3000 8001 5432 9999 3001 4000 5000 8082 54321)
  for port in "${ports[@]}"; do
    if ! ufw status | grep -q "$port/tcp"; then
      ufw allow "$port/tcp" comment "Supabase $port"
      log "Port $port/tcp ouvert pour Supabase."
    fi
  done
  ufw reload
  ok "UFW configurÃ© - Ports Supabase ouverts."
}

# Nettoyage des ressources prÃ©cÃ©dentes (conteneurs, rÃ©seaux, ports)
cleanup_previous() {
  log "ðŸ§¹ Nettoyage des ressources rÃ©siduelles Supabase..."
  if [[ -d "$PROJECT_DIR" ]]; then
    cd "$PROJECT_DIR"
    su "$TARGET_USER" -c "docker compose down -v --remove-orphans" 2>/dev/null || true
    cd ~ || true
    sudo rm -rf "$PROJECT_DIR"
    log "Dossier prÃ©cÃ©dent supprimÃ©: $PROJECT_DIR"
  fi
  docker rm -f "$(docker ps -a -q --filter "name=supabase-" 2>/dev/null)" 2>/dev/null || true
  docker network rm supabase_default 2>/dev/null || true
  docker network prune -f 2>/dev/null || true
  local ports=(3000 8001 5432 9999 3001 4000 5000 8082 54321)
  for port in "${ports[@]}"; do
    if command -v lsof &> /dev/null && sudo lsof -i :"$port" &> /dev/null; then
      log "LibÃ©ration port $port..."
      sudo kill -9 "$(sudo lsof -t -i :"$port")" 2>/dev/null || true
    fi
  done
  ok "Nettoyage terminÃ© - Ports et ressources libÃ©rÃ©s."
}

# CrÃ©ation du dossier projet et volumes (sans clone Realtime - non nÃ©cessaire avec SEED_SELF_HOST)
setup_project_dir() {
  log "ðŸ“ CrÃ©ation du dossier projet et volumes..."
  mkdir -p "$(dirname "$PROJECT_DIR")"
  mkdir -p "$PROJECT_DIR/volumes/db" "$PROJECT_DIR/volumes/auth" "$PROJECT_DIR/volumes/storage" "$PROJECT_DIR/volumes/functions/main" "$PROJECT_DIR/volumes/kong/logs" "$PROJECT_DIR/volumes/kong/conf"
  sudo chown -R "$TARGET_USER:$TARGET_USER" "$PROJECT_DIR"
  sudo chmod -R 777 "$PROJECT_DIR/volumes"
  local function_file="$PROJECT_DIR/volumes/functions/main/index.ts"
  echo 'export default async function handler(req) { return new Response("Hello from Edge!"); }' > "$function_file"
  if [[ ! -f "$function_file" ]]; then
    error "Ã‰chec crÃ©ation fichier index.ts: $function_file"
  fi
  cd "$PROJECT_DIR" || error "Impossible d'accÃ©der au dossier projet: $PROJECT_DIR"
  ok "Dossier projet prÃªt: $(pwd) | Volumes configurÃ©s (sans clone Realtime)."
}

# GÃ©nÃ©ration ou rÃ©utilisation des secrets (JWT en base64 32 pour fix healthcheck) - Backup dÃ©placÃ©
generate_secrets() {
  log "ðŸ” GÃ©nÃ©ration ou rÃ©utilisation des secrets Supabase..."
  local backup_date=$(date +%Y%m%d)
  local backup_file="/home/$TARGET_USER/supabase-secrets-backup-${backup_date}.env"
  if [[ -f "$backup_file" && "$FORCE_RECREATE" != "1" ]]; then
    log "RÃ©utilisation .env de backup rÃ©cent: $backup_file"
    cp "$backup_file" "$PROJECT_DIR/.env"
    source "$PROJECT_DIR/.env"
    if [[ -z "${DB_ENC_KEY:-}" ]]; then
      DB_ENC_KEY=$(openssl rand -hex 8)
      export DB_ENC_KEY
      sed -i "/^DB_ENC_KEY=/d" "$PROJECT_DIR/.env"
      echo "DB_ENC_KEY=$DB_ENC_KEY" >> "$PROJECT_DIR/.env"
    fi
    if [[ -z "${REALTIME_SECRET_KEY_BASE:-}" ]]; then
      REALTIME_SECRET_KEY_BASE="$DB_ENC_KEY"
      export REALTIME_SECRET_KEY_BASE
      sed -i "/^REALTIME_SECRET_KEY_BASE=/d" "$PROJECT_DIR/.env"
      echo "REALTIME_SECRET_KEY_BASE=$REALTIME_SECRET_KEY_BASE" >> "$PROJECT_DIR/.env"
    fi
    local critical_vars=(POSTGRES_PASSWORD JWT_SECRET ANON_KEY SERVICE_ROLE_KEY API_EXTERNAL_URL SUPABASE_PUBLIC_URL DB_ENC_KEY)
    for var in "${critical_vars[@]}"; do
      if [[ -z "${!var:-}" ]]; then
        error "Variable critique manquante dans backup: $var"
      fi
    done
    log "Secrets chargÃ©s depuis backup (JWT prefix: ${JWT_SECRET:0:8}... | DB_ENC_KEY: ${DB_ENC_KEY:0:8}...)"
  else
    log "GÃ©nÃ©ration de nouveaux secrets sÃ©curisÃ©s (JWT base64 32 pour fix healthcheck)..."
    POSTGRES_PASSWORD=$(openssl rand -base64 32 | tr -d '=+/' | cut -c1-32)
    JWT_SECRET=$(openssl rand -base64 32 | tr -d '=+/' | cut -c1-32)  # Base64 32 chars (fix 403)
    DB_ENC_KEY=$(openssl rand -hex 8)
    local site_url="http://$(hostname -I | awk '{print $1}'):${SUPABASE_PORT}"
    ANON_KEY=$(echo -n "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6IiIsInJvbGUiOiJhbm9uIiwiaWF0IjoxNDk4MTAwODAwLCJleHAiOjE4MTc0ODQ4MDB9.${JWT_SECRET}" | base64 -w0)
    SERVICE_ROLE_KEY=$(echo -n "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6IiIsInJvbGUiOiJzZXJ2aWNlX3JvbGUiLCJpYXQiOjE0OTgxMDA4MDAsImV4cCI6MTgxNzQ4NDgwMH0.${JWT_SECRET}" | base64 -w0)
    REALTIME_SECRET_KEY_BASE="$DB_ENC_KEY"
    API_EXTERNAL_URL="$site_url"
    SUPABASE_PUBLIC_URL="$site_url"
    export POSTGRES_PASSWORD JWT_SECRET DB_ENC_KEY REALTIME_SECRET_KEY_BASE ANON_KEY SERVICE_ROLE_KEY API_EXTERNAL_URL SUPABASE_PUBLIC_URL APP_NAME
    log "Secrets gÃ©nÃ©rÃ©s (JWT prefix: ${JWT_SECRET:0:8}... | DB_ENC_KEY: ${DB_ENC_KEY:0:8}...)"
    # Backup reportÃ© Ã  create_env_file (v2.6.16)
  fi
  ok "Secrets prÃªts (backup post-.env)."
}

# Validation fichier post-here doc (Ã©tendue v2.6.16)
validate_file() {
  local file="$1"
  local check_var="$2"
  if [[ ! -f "$file" ]] || ! grep -q "^$check_var=" "$file"; then
    error "Ã‰chec validation $file (manque $check_var)"
  fi
}

# CrÃ©ation du fichier .env optimisÃ© pour ARM64 + backup (GOTRUE_API_PORT=9999 v2.6.21)
create_env_file() {
  log "ðŸ“„ CrÃ©ation du fichier .env optimisÃ© pour Pi5 ARM64..."
  local vars=(POSTGRES_PASSWORD JWT_SECRET DB_ENC_KEY REALTIME_SECRET_KEY_BASE ANON_KEY SERVICE_ROLE_KEY API_EXTERNAL_URL SUPABASE_PUBLIC_URL)
  for var in "${vars[@]}"; do
    if [[ -z "${!var:-}" ]]; then
      error "Variable manquante pour .env: $var"
    fi
  done
  local dashboard_pass=$(openssl rand -base64 16 | tr -d '=+/')
  local backup_date=$(date +%Y%m%d)
  local backup_file="/home/$TARGET_USER/supabase-secrets-backup-${backup_date}.env"
  # Set +e local pour tolÃ©rer warning heredoc mineur
  set +e
  cat > "$PROJECT_DIR/.env" << ENV
# Supabase .env - OptimisÃ© Pi5 ARM64 (GOTRUE_API_PORT=9999 v2.6.21)
POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
JWT_SECRET=${JWT_SECRET}
DB_ENC_KEY=${DB_ENC_KEY}
REALTIME_SECRET_KEY_BASE=${REALTIME_SECRET_KEY_BASE}
APP_NAME=${APP_NAME}
ANON_KEY=${ANON_KEY}
SERVICE_ROLE_KEY=${SERVICE_ROLE_KEY}
DASHBOARD_USERNAME=supabase
DASHBOARD_PASSWORD=${dashboard_pass}
API_EXTERNAL_URL=${API_EXTERNAL_URL}
SUPABASE_PUBLIC_URL=${SUPABASE_PUBLIC_URL}
POSTGRES_HOST=postgresql
POSTGRES_DB=postgres
POSTGRES_PORT=5432
KONG_HTTP_PORT=${SUPABASE_PORT}
PGRST_DB_SCHEMAS=public,graphql_public,storage,supabase_functions
PGRST_DB_ANON_ROLE=anon
PGRST_JWT_SECRET=${JWT_SECRET}
POSTGRES_SHARED_PRELOAD_LIBRARIES=timescaledb,pg_stat_statements,pgcrypto
POSTGRES_SHARED_BUFFERS=128MB
POSTGRES_WORK_MEM=8MB
POSTGRES_MAINTENANCE_WORK_MEM=32MB
POSTGRES_MAX_CONNECTIONS=25
REALTIME_DB_ENC_KEY=${DB_ENC_KEY}
REALTIME_JWT_SECRET=${JWT_SECRET}
REALTIME_ULIMIT_NOFILE=262144
RLIMIT_NOFILE=262144
GOTRUE_JWT_SECRET=${JWT_SECRET}
GOTRUE_SITE_URL=${SUPABASE_PUBLIC_URL}
GOTRUE_API_EXTERNAL_URL=${API_EXTERNAL_URL}
GOTRUE_API_PORT=9999  # v2.6.21: Fix listen port pour health/curl
GOTRUE_DB_DRIVER=postgres
GOTRUE_DISABLE_SIGNUP=false
STORAGE_BACKEND=file
FILE_STORAGE_BACKEND_PATH=/var/lib/storage
ENABLE_IMAGE_TRANSFORMATION=false  # v2.6.19: Disabled sans imgproxy
IMGPROXY_URL=  # Vide sans service imgproxy
EDGE_RUNTIME_JWT_SECRET=${JWT_SECRET}
SEED_SELF_HOST=true
ENV
  set -e  # Restaure set -e
  chown "$TARGET_USER:$TARGET_USER" "$PROJECT_DIR/.env"
  # Validation Ã©tendue (toutes vars critiques, v2.6.16)
  for var in "${vars[@]}"; do
    validate_file "$PROJECT_DIR/.env" "$var"
  done
  validate_file "$PROJECT_DIR/.env" "DASHBOARD_PASSWORD"
  validate_file "$PROJECT_DIR/.env" "GOTRUE_API_PORT"  # v2.6.21
  # Backup ici (post-crÃ©ation .env, v2.6.16)
  cp "$PROJECT_DIR/.env" "$backup_file"
  echo "APP_NAME=$APP_NAME" >> "$backup_file"
  log "Backup crÃ©Ã©: $backup_file"
  ok ".env crÃ©Ã© - GOTRUE_API_PORT=9999 + realtime schema (buffers 128MB, backup OK)."
}

# CrÃ©ation du fichier kong.yml pour configuration declarative (v2.6.18)
create_kong_config() {
  log "ðŸŒ CrÃ©ation configuration Kong declarative (kong.yml)..."
  cat > "$PROJECT_DIR/kong.yml" << 'KONG_YML'
_format_version: "2.1"
services:
  - name: auth
    url: http://auth:9999
    routes:
      - name: auth-route
        paths: ["/auth/v1/"]
  - name: rest
    url: http://rest:3000
    routes:
      - name: rest-route
        paths: ["/rest/v1/"]
  - name: storage
    url: http://storage:5000
    routes:
      - name: storage-route
        paths: ["/storage/v1/"]
  - name: realtime
    url: http://realtime:4000/socket
    routes:
      - name: realtime-route
        paths: ["/realtime/v1/"]
  - name: meta
    url: http://meta:8080
    routes:
      - name: meta-route
        paths: ["/pg/"]
KONG_YML
  chown "$TARGET_USER:$TARGET_USER" "$PROJECT_DIR/kong.yml"
  ok "kong.yml crÃ©Ã© - Routes pour auth/rest/storage/realtime/meta."
}

# CrÃ©ation du fichier docker-compose.yml (GOTRUE_API_PORT via env; realtime DB_SCHEMA + search_path v2.6.21)
create_docker_compose() {
  log "ðŸ³ CrÃ©ation et validation docker-compose.yml..."
  cat > "$PROJECT_DIR/docker-compose.yml" << 'COMPOSE'
services:
  postgresql:
    image: supabase/postgres:15.1.0.147
    ports:
      - "5432:5432"
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./volumes/db:/var/lib/postgresql/data
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
  kong:
    image: kong:3.4.0
    ports:
      - "${KONG_HTTP_PORT:-8001}:8000"
    environment:
      KONG_DATABASE: off
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_PREFIX: /var/run/kong_prefix
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_DECLARATIVE_CONFIG: /etc/kong/kong.yml
    volumes:
      - ./volumes/kong:/var/run/kong_prefix
      - ./kong.yml:/etc/kong/kong.yml:ro  # v2.6.18: Inject config declarative
    depends_on:
      postgresql:
        condition: service_started  # v2.6.20: RelaxÃ© pour boot lent
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/status"]  # v2.6.20: Admin status
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
  auth:
    image: supabase/gotrue:v2.153.0
    depends_on:
      postgresql:
        condition: service_started  # v2.6.20: RelaxÃ© pour migrations
    environment:
      GOTRUE_JWT_SECRET: ${JWT_SECRET}
      GOTRUE_SITE_URL: ${SUPABASE_PUBLIC_URL}
      GOTRUE_API_EXTERNAL_URL: ${API_EXTERNAL_URL}
      API_EXTERNAL_URL: ${API_EXTERNAL_URL}
      DATABASE_URL: postgres://supabase_admin:${POSTGRES_PASSWORD}@postgresql:5432/postgres  # supabase_admin pour migrations
      GOTRUE_DB_DRIVER: postgres
      GOTRUE_API_PORT: ${GOTRUE_API_PORT:-9999}  # v2.6.21: Via .env
    ports:
      - "9999:9999"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9999/health"]
      interval: 10s
      timeout: 5s
      retries: 10  # Ã‰tendu pour migrations
      start_period: 60s  # Temps pour migrations auth
  rest:
    image: postgrest/postgrest:v12.0.2
    depends_on:
      postgresql:
        condition: service_started  # v2.6.20: RelaxÃ©
    environment:
      PGRST_DB_URI: postgres://supabase_admin:${POSTGRES_PASSWORD}@postgresql:5432/postgres  # supabase_admin
      PGRST_JWT_SECRET: ${JWT_SECRET}
      PGRST_DB_SCHEMAS: public,storage
      PGRST_DB_ANON_ROLE: anon
    ports:
      - "3001:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
  realtime:
    image: supabase/realtime:${REALTIME_VERSION}  # Officielle ARM64
    depends_on:
      postgresql:
        condition: service_started  # v2.6.20: RelaxÃ©
    environment:
      APP_NAME: ${APP_NAME}
      SEED_SELF_HOST: true  # Seeding via image
      DB_SCHEMA: realtime  # v2.6.21: Match migrations Ecto
      DB_AFTER_CONNECT_QUERY: 'SET search_path TO public, private, realtime'  # v2.6.21: Ã‰tendu pour compat
      SLOT_NAME: realtime
      PUBLICATIONS: '["supabase_realtime"]'
      DNS_NODES: "''"
      DB_HOST: postgresql
      DB_PORT: 5432
      DB_USER: postgres  # postgres pour realtime
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_NAME: postgres
      DB_SSL: false
      JWT_SECRET: ${JWT_SECRET}
      SECRET_KEY_BASE: ${REALTIME_SECRET_KEY_BASE}
      DB_ENC_KEY: ${DB_ENC_KEY}
      PORT: 4000
      HOSTNAME: 0.0.0.0
      ERL_AFLAGS: "-proto_dist inet_tcp"
      RLIMIT_NOFILE: 262144
      SECURE_CHANNELS: true
      EXPOSE_METRICS: false
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    ports:
      - "4000:4000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 120s  # Boot Elixir ARM64
  storage:
    image: supabase/storage-api:v1.0.8
    depends_on:
      auth:
        condition: service_started  # v2.6.20: RelaxÃ©
      rest:
        condition: service_started
      realtime:
        condition: service_started
    environment:
      ANON_KEY: ${ANON_KEY}
      SERVICE_ROLE_KEY: ${SERVICE_ROLE_KEY}
      POSTGREST_URL: http://rest:3000
      PGRST_JWT_SECRET: ${JWT_SECRET}
      DATABASE_URL: postgres://postgres:${POSTGRES_PASSWORD}@postgresql:5432/postgres  # postgres pour storage
      FILE_STORAGE_BACKEND: file
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      STORAGE_BACKEND: file
      TENANT_ID: stub
      REGION: stub
      GLOBAL_S3_BUCKET: stub
      ENABLE_IMAGE_TRANSFORMATION: "false"  # Disabled sans imgproxy
    ports:
      - "5000:5000"
    volumes:
      - ./volumes/storage:/var/lib/storage
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s  # Temps pour deps
  studio:
    image: supabase/studio:latest
    depends_on:
      auth:
        condition: service_started  # v2.6.20: RelaxÃ©
      rest:
        condition: service_started
      kong:
        condition: service_started
    ports:
      - "3000:3000"
    environment:
      SUPABASE_URL: http://kong:8000
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SERVICE_ROLE_KEY}
      SUPABASE_PUBLIC_URL: ${SUPABASE_PUBLIC_URL}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 60s
  meta:
    image: supabase/postgres-meta:v0.82.0
    depends_on:
      postgresql:
        condition: service_started  # v2.6.20: RelaxÃ©
    environment:
      PG_META_DB_HOST: postgresql
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD}  # User postgres implicite
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]  # Sans pg_isready
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
  edge-functions:
    image: supabase/edge-runtime:v1.57.1
    depends_on:
      auth:
        condition: service_started  # v2.6.20: RelaxÃ©
    environment:
      JWT_SECRET: ${JWT_SECRET}
      SUPABASE_URL: http://kong:8000
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SERVICE_ROLE_KEY}
      VERIFY_JWT: ${JWT_SECRET}
    volumes:
      - ./volumes/functions:/home/deno/functions
    ports:
      - "54321:9000"
    command: ["start", "--main-service", "/home/deno/functions/main"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
COMPOSE
  # Remplace REALTIME_VERSION dans yml
  sed -i "s|\${REALTIME_VERSION}|$REALTIME_VERSION|g" "$PROJECT_DIR/docker-compose.yml"
  chown "$TARGET_USER:$TARGET_USER" "$PROJECT_DIR/docker-compose.yml"
  if ! su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose config" &> /dev/null; then
    error "Erreur de validation docker-compose.yml"
  fi
  ok "docker-compose.yml crÃ©Ã© - GOTRUE_API_PORT via env + realtime DB_SCHEMA/search_path."
}

# Fonctions utilitaires pour unhealthy/exited + relance Ã©tendue (+5)
get_unhealthy_services() {
  local project_dir="$1"
  su "$TARGET_USER" -c "cd '$project_dir' && docker compose ps --all --format '{{.Name}} {{.Status}}'" | \
    awk '{ if ($2 ~ /unhealthy/) { print $1 } }' | tr '\n' ' ' | sed 's/ $//' || true
}

get_exited_services() {
  local project_dir="$1"
  su "$TARGET_USER" -c "cd '$project_dir' && docker compose ps --filter 'status=exited' --format '{{.Name}}'" | \
    tr '\n' ' ' | sed 's/ $//' || true
}

# PrÃ©-tÃ©lÃ©chargement des images Docker (incl Realtime officielle)
pre_pull_images() {
  log "ðŸ” PrÃ©-tÃ©lÃ©chargement des images critiques (ARM64 compatibles)..."
  local images=(
    "supabase/postgres:15.1.0.147"
    "kong:3.4.0"
    "supabase/gotrue:v2.153.0"
    "postgrest/postgrest:v12.0.2"
    "supabase/realtime:$REALTIME_VERSION"
    "supabase/storage-api:v1.0.8"
    "supabase/studio:latest"
    "supabase/postgres-meta:v0.82.0"
    "supabase/edge-runtime:v1.57.1"
  )
  for image in "${images[@]}"; do
    log "TÃ©lÃ©chargement de $image..."
    if ! docker pull "$image" &> /dev/null; then
      error "Ã‰chec pull $image - VÃ©rifiez internet/ARM64 compat."
    fi
    ok "Image $image tÃ©lÃ©chargÃ©e."
  done
  ok "Images prÃ©-tÃ©lÃ©chargÃ©es."
}

# CrÃ©ation robuste de schÃ©mas (REASSIGN + ALTER OWNER pour fix realtime; v2.6.21-fix)
create_schema_robust() {
  local schema_name="$1"
  local owner="postgres"  # Owner pour Realtime/PG
  log "Configuration schÃ©ma $schema_name (CREATE + GRANT via postgres, retry 3x)..."
  for attempt in {1..3}; do
    # Solution: Utiliser postgres et ignorer l'erreur de propriÃ©taire si schÃ©ma existe dÃ©jÃ 
    local create_cmd="CREATE SCHEMA IF NOT EXISTS $schema_name;"
    local grant_cmd="GRANT USAGE ON SCHEMA $schema_name TO postgres, service_role, anon, authenticated; GRANT ALL ON ALL TABLES IN SCHEMA $schema_name TO postgres, service_role;"
    local output=$(su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose exec -T postgresql psql -U postgres -d postgres -c '$create_cmd $grant_cmd'" 2>&1) || true
    log "Output superuser/psql (attempt $attempt): $output"
    if echo "$output" | grep -q "CREATE SCHEMA" || echo "$output" | grep -q "already exists" || echo "$output" | grep -q "GRANT"; then
      ok "SchÃ©ma $schema_name DROP/CREATE/OWNER fixÃ©."
      return 0
    fi
    warn "Ã‰chec attempt $attempt pour $schema_name - Retry..."
    sleep 5
  done
  error "Ã‰chec crÃ©ation schÃ©ma $schema_name aprÃ¨s 3 tentatives - VÃ©rifiez logs PG."
}

# Initialisation migrations (realtime WAL; DROP/CREATE schÃ©ma v2.6.21)
init_auth_migrations() {
  log "ðŸ”§ Initialisation migrations (realtime WAL; auth gÃ©rÃ© par images)..."
  su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose up -d postgresql"
  # Boucle pg_isready Ã©tendue
  for i in {1..20}; do
    if su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose exec -T postgresql pg_isready -U postgres" > /dev/null 2>&1; then
      ok "PG ready aprÃ¨s $i tentatives."
      break
    fi
    sleep 5
    [[ $i -eq 20 ]] && error "PG non ready aprÃ¨s 100s - VÃ©rifiez logs postgresql."
  done
  # SchÃ©ma realtime avec DROP CASCADE (fix owner/perms v2.6.21)
  create_schema_robust "realtime"
  # Publication WAL avec DROP
  if ! su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose exec -T postgresql psql -U postgres -d postgres -c 'DROP PUBLICATION IF EXISTS supabase_realtime; CREATE PUBLICATION supabase_realtime FOR ALL TABLES;'" &> /dev/null; then
    warn "Ã‰chec publication WAL (non fatal) - VÃ©rifiez logs postgresql."
    su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose exec -T postgresql psql -U postgres -d postgres -c 'ALTER PUBLICATION supabase_realtime OWNER TO postgres;'" &> /dev/null || true
  fi
  # Pas de migrations Realtime manuel (SEED_SELF_HOST=true gÃ¨re)
  log "Migrations via images officielles (auth/realtime WAL)."
  sleep 30  # Attente post-init
  ok "Initialisation OK - Seeding via images (schÃ©ma realtime fixÃ©)."
}

# Validation routes Kong (curl tests post-up, sleep 120s v2.6.20)
validate_kong_routes() {
  log "ðŸŒ Validation routes Kong (curl /auth/v1/, /rest/v1/, etc.)..."
  sleep 120  # v2.6.20: Ã‰tendu pour migrations + boot
  local routes=( "/auth/v1/" "/rest/v1/" "/storage/v1/" "/pg/" "/realtime/v1/" )
  for route in "${routes[@]}"; do
    for attempt in {1..5}; do  # Ã‰tendu Ã  5 pour migrations
      local response_code=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:${SUPABASE_PORT}${route}" 2>/dev/null || echo 0)
      if [[ "$response_code" == "200" || "$response_code" == "401" || "$response_code" == "404" || "$response_code" == "503" ]]; then  # TolÃ¨re 503 boot
        ok "Route $route OK (code $response_code, attempt $attempt)."
        break
      fi
      warn "Route $route KO (code $response_code, attempt $attempt) - Retry..."
      sleep 10
    done
    [[ $attempt -le 5 ]] || warn "Route $route persistante KO - VÃ©rifiez logs."
  done
  ok "Routes Kong validÃ©es."
}

# DÃ©ploiement principal (up + validation Kong + relance realtime si exited v2.6.21)
deploy_supabase() {
  log "ðŸš€ DÃ©ploiement complet de Supabase..."
  pre_pull_images
  init_auth_migrations
  if ! su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose up -d --pull always"; then
    error "Ã‰chec up - Logs: docker compose logs."
  fi
  sleep 180  # Boot ARM64 + migrations
  # Relance realtime si exited (fix crash perms v2.6.21)
  local exited=$(get_exited_services "$PROJECT_DIR")
  if [[ $exited =~ "realtime" ]]; then
    warn "Realtime exited - Relance post-up..."
    su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart realtime"
    sleep 30
  fi
  validate_kong_routes  # v2.6.18: Post-up
  log "â³ Attente healthchecks (360s, relance +5)..."
  local max_wait=72
  local relance_count=0
  for i in $(seq 1 "$max_wait"); do
    sleep 5
    local status_output=$(su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose ps --all --format 'table {{.Name}}\t{{.Status}}'")
    log "Status $i/$max_wait:\n$status_output"
    local unhealthy=$(get_unhealthy_services "$PROJECT_DIR")
    if [[ -z "$unhealthy" ]]; then
      break
    fi
    relance_count=$((relance_count + 1))
    warn "Unhealthy: $unhealthy - Relance #$relance_count (tolÃ¨re realtime)..."
    [[ "$unhealthy" =~ "realtime" ]] && su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart realtime || true"
    [[ "$unhealthy" =~ "storage" ]] && su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart storage || true"
    [[ "$unhealthy" =~ "auth" ]] && su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart auth || true"  # v2.6.21: TolÃ¨re auth boot
    su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart $unhealthy || true"
    if [[ $relance_count -ge 5 ]]; then
      warn "Max relances atteintes - Continue malgrÃ© unhealthy."
      break
    fi
    if [[ $i -eq $max_wait ]]; then
      warn "Timeout - Relance finale realtime/auth/storage..."
      su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart realtime auth storage || true"
    fi
  done
  ok "DÃ©ployÃ© - VÃ©rifiez: docker compose ps --all"
}

# Validation (curl + tolÃ©rance realtime/auth)
validate_deployment() {
  log "ðŸ§ª Validation finale..."
  sleep 120
  local ip=$(hostname -I | awk '{print $1}')
  # Studio
  for i in {1..10}; do  # Ã‰tendu pour stabiliser
    if curl -s "http://localhost:3000" > /dev/null; then
      ok "Studio OK - http://$ip:3000"
      break
    fi
    warn "Studio attente ($i/10)..."
    sleep 10
  done
  [[ $i -le 10 ]] || error "Studio KO - Logs studio."
  # API
  for i in {1..5}; do
    if curl -s "http://localhost:$SUPABASE_PORT" > /dev/null; then
      ok "API OK - http://$ip:$SUPABASE_PORT"
      break
    fi
    warn "API attente ($i/5)..."
    sleep 5
  done
  [[ $i -le 5 ]] || error "API KO - Logs kong."
  # PG
  if ! su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose exec -T postgresql pg_isready -U postgres" > /dev/null; then
    error "PG KO - Logs postgresql."
  fi
  ok "PG connectÃ©."
  # Unhealthy (tolÃ¨re realtime/auth)
  local unhealthy=$(get_unhealthy_services "$PROJECT_DIR")
  if [[ -n "$unhealthy" && ! "$unhealthy" =~ "(realtime|auth)" ]]; then
    warn "Unhealthy non-tolÃ©rÃ©: $unhealthy - Relance..."
    su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart $unhealthy"
  fi
  # Realtime
  for i in {1..3}; do
    if curl -s "http://localhost:4000/health" > /dev/null; then
      ok "Realtime OK (port 4000)"
      break
    fi
    warn "Realtime curl KO ($i/3) - Logs realtime."
    sleep 10
  done
  # Exited
  local exited=$(get_exited_services "$PROJECT_DIR")
  if [[ -n "$exited" ]]; then
    warn "Exited: $exited - Relance..."
    for svc in $exited; do
      su "$TARGET_USER" -c "cd '$PROJECT_DIR' && docker compose restart $svc" || true
      log "RelancÃ©: $svc"
    done
  fi
  # Keys
  log "ðŸ”‘ API Keys (sauvegardez-les!):"
  grep -E "ANON_KEY|SERVICE_ROLE_KEY" "$PROJECT_DIR/.env" | sed 's/^/   /'
  log "Dashboard: http://$ip:3000 | User: supabase | Pass: (dans .env)"
  ok "Validation OK! Supabase opÃ©rationnel."
}

# Main
main() {
  require_root
  check_prereqs
  activate_ufw
  cleanup_previous
  setup_project_dir
  generate_secrets
  create_env_file
  create_kong_config  # v2.6.18
  create_docker_compose
  deploy_supabase
  validate_deployment
  log "ðŸŽ‰ Supabase installÃ©!"
  log "ðŸ“‹ Logs: $LOG_FILE"
  log "ðŸš€ Post-install:"
  log "   1. http://$(hostname -I | awk '{print $1}'):3000"
  log "   2. CrÃ©ez projet dans Studio (attendez 3-5 min pour migrations)."
  log "   3. Notez clÃ©s .env."
  log "   4. ArrÃªt/redÃ©marrage: cd $PROJECT_DIR && docker compose down/up -d"
  log "   5. Unhealthy persistant? docker compose logs <service> | grep ERROR"
  log "   6. Recreate: sudo FORCE_RECREATE=1 $0"
}

main "$@"