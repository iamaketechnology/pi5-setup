#!/bin/bash
# =============================================================================
# Script 4 : Fix des Services Unhealthy pour Supabase Self-Hosted sur Raspberry Pi 5 (ARM64, 16GB RAM)
# Auteur : IngÃ©nieur DevOps ARM64 - OptimisÃ© pour Bookworm 64-bit (Kernel 6.12+)
# Version : 1.0.1 (Fix: Logs /tmp + dÃ©tection unhealthy corrigÃ©e)
# Objectif : Diagnostiquer et corriger les services unhealthy dans Supabase (ex: auth, realtime, storage, meta, edge-functions) sans redÃ©ployer l'ensemble.
# PrÃ©-requis : Script 3 exÃ©cutÃ© avec succÃ¨s (Docker Compose up). ExÃ©cuter en tant que user pi (non sudo, car Docker group ajoutÃ©).
# Usage : cd /home/pi/stacks/supabase && ./fix-supabase-unhealthy.sh
# Actions Post-Script : VÃ©rifiez `docker compose ps` ; si persistant, relancez ce script ou consultez logs spÃ©cifiques.
# Notes :
# - Relance sÃ©lective basÃ©e sur status (tolÃ¨re PG healthy).
# - Diagnostics : Logs tail=20 pour services KO, avec grep ERROR/WARN.
# - Validation : Boucle healthcheck jusqu'Ã  5 min (relance si KO).
# - Si realtime crash persiste (perms schÃ©ma), force DROP/CREATE via PG.
# =============================================================================
set -euo pipefail  # ArrÃªt sur erreur, undefined vars, pipefail

# Fonctions de logging colorÃ©es pour traÃ§abilitÃ©
log()  { echo -e "\033[1;36m[FIX-SUPABASE]\033[0m $*"; }
warn() { echo -e "\033[1;33m[WARN]\033[0m $*"; }
ok()   { echo -e "\033[1;32m[OK]\033[0m $*"; }
error() { echo -e "\033[1;31m[ERROR]\033[0m $*"; exit 1; }

# Variables globales
PROJECT_DIR="/home/${USER}/stacks/supabase"  # AssumÃ© pi
LOG_FILE="/tmp/supabase-fix-unhealthy-$(date +%Y%m%d_%H%M%S).log"  # /tmp au lieu de /var/log
MAX_RETRIES=5  # Relances max par service
HEALTH_TIMEOUT=300  # 5 min total pour healthchecks

# Redirection des logs vers fichier pour audit (stdout + fichier)
exec 1> >(tee -a "$LOG_FILE")
exec 2> >(tee -a "$LOG_FILE" >&2)
log "=== DÃ©but Fix Services Unhealthy v1.0.1 - $(date) ==="
log "Projet: $PROJECT_DIR | User: $USER"

# VÃ©rification dossier projet et Docker Compose
check_prereqs() {
  log "ðŸ” VÃ©rification prÃ©-requis..."
  if [[ ! -d "$PROJECT_DIR" ]]; then
    error "Dossier projet manquant: $PROJECT_DIR - ExÃ©cutez Script 3 d'abord."
  fi
  cd "$PROJECT_DIR" || error "Impossible d'accÃ©der Ã  $PROJECT_DIR"
  if ! command -v docker &> /dev/null; then
    error "Docker manquant - Ajoutez user au groupe docker et reloguez."
  fi
  if ! docker compose version | grep -q "2\."; then
    error "Docker Compose v2+ requis."
  fi
  if ! docker compose ps --all | grep -q "supabase-"; then
    warn "Aucun conteneur Supabase dÃ©tectÃ© - Lancez docker compose up -d d'abord."
  fi
  ok "PrÃ©-requis validÃ©s."
}

# Fonction pour obtenir services unhealthy/exited
get_unhealthy_services() {
  docker compose ps --all --format '{{.Name}}\t{{.Status}}' | grep -E "(unhealthy)" | awk '{print $1}' | sed 's/supabase-//' | sed 's/-1$//' | tr '\n' ' ' | sed 's/ $//' || true
}

get_exited_services() {
  docker compose ps --filter 'status=exited' --format '{{.Name}}' | tr '\n' ' ' | sed 's/ $//' || true
}

# Diagnostics logs pour un service (tail=20 + grep ERROR/WARN)
diagnose_service() {
  local service="$1"
  log "ðŸ” Diagnostics pour $service..."
  docker compose logs "$service" --tail=20 | tee -a "$LOG_FILE"
  if docker compose logs "$service" 2>&1 | grep -qE "(ERROR|WARN|permission denied|connection reset)"; then
    warn "Erreurs dÃ©tectÃ©es dans logs $service - Consultez $LOG_FILE pour dÃ©tails."
  else
    ok "Logs $service clean (pas d'ERROR/WARN rÃ©cents)."
  fi
}

# Relance un service + validation healthcheck (retry jusqu'Ã  timeout)
restart_and_validate() {
  local service="$1"
  local health_cmd="$2"  # Ex: curl -f http://localhost:9999/health pour auth
  local max_wait=60  # 1 min par service
  log "ðŸ”„ Relance $service + validation healthcheck..."
  docker compose restart "$service"
  sleep 10  # Attente boot initial
  local start_time=$(date +%s)
  for i in $(seq 1 "$max_wait"); do
    sleep 1
    if eval "$health_cmd" > /dev/null 2>&1; then
      ok "$service healthy aprÃ¨s $i s."
      return 0
    fi
    if [[ $(( $(date +%s) - start_time )) -ge $HEALTH_TIMEOUT ]]; then
      warn "$service timeout healthcheck - Persiste unhealthy."
      return 1
    fi
  done
  return 1
}

# Fix spÃ©cifique realtime (si perms schÃ©ma)
fix_realtime_perms() {
  local service="realtime"
  if ! docker compose ps | grep -q "$service.*Up"; then
    warn "$service non up - Relance d'abord."
    return 1
  fi
  log "ðŸ› ï¸ Fix perms schÃ©ma realtime (DROP/CREATE/GRANT)..."
  # Via PG superuser (postgres sans pass)
  docker compose exec -T postgresql psql -U postgres -d postgres -c "
    DROP SCHEMA IF EXISTS realtime CASCADE;
    CREATE SCHEMA realtime;
    ALTER SCHEMA realtime OWNER TO postgres;
    GRANT ALL ON SCHEMA realtime TO postgres;
    GRANT USAGE, CREATE ON SCHEMA realtime TO postgres;
  " 2>&1 | tee -a "$LOG_FILE" || warn "Ã‰chec fix perms realtime - VÃ©rifiez logs PG."
  ok "Perms realtime fixÃ© - Relance service."
  docker compose restart realtime
  sleep 20
}

# Fix spÃ©cifique auth (si connection reset)
fix_auth_port() {
  local service="auth"
  log "ðŸ› ï¸ VÃ©rif/fix GOTRUE_API_PORT pour $service..."
  if ! grep -q "^GOTRUE_API_PORT=9999$" .env; then
    warn "GOTRUE_API_PORT manquant dans .env - Ajoutez manuellement: echo 'GOTRUE_API_PORT=9999' >> .env"
    return 1
  fi
  docker compose logs "$service" | grep -q "GoTrue API started on: :9999" && ok "Port auth OK." || {
    warn "Port auth KO - Relance avec env."
    docker compose up -d --force-recreate "$service"
    sleep 10
  }
}

# Main fix loop
main_fix() {
  check_prereqs
  local unhealthy=$(get_unhealthy_services)
  local exited=$(get_exited_services)
  if [[ -z "$unhealthy" && -z "$exited" ]]; then
    ok "Tous services healthy - Rien Ã  fixer."
    exit 0
  fi
  log "ðŸš¨ Services Ã  fixer: Unhealthy=$unhealthy | Exited=$exited"

  # Fix exited d'abord
  if [[ -n "$exited" ]]; then
    for svc in $exited; do
      diagnose_service "${svc#supabase-}"  # Strip prefix
      docker compose restart "${svc#supabase-}"
      sleep 10
    done
  fi

  # Fix unhealthy sÃ©lectifs
  for svc in auth realtime storage meta edge-functions kong studio rest; do  # Ordre deps (auth/realtime d'abord)
    if [[ "$unhealthy" =~ "$svc" || "$exited" =~ "$svc" ]]; then
      diagnose_service "$svc"
      case "$svc" in
        realtime)
          fix_realtime_perms
          restart_and_validate realtime 'curl -f http://localhost:4000/health'
          ;;
        auth)
          fix_auth_port
          restart_and_validate auth 'curl -f http://localhost:9999/health'
          ;;
        storage)
          restart_and_validate storage 'curl -f http://localhost:5000/health'
          ;;
        meta)
          restart_and_validate meta 'curl -f http://localhost:8080/health'
          ;;
        edge-functions)
          restart_and_validate edge-functions 'curl -f http://localhost:54321/health'
          ;;
        kong)
          restart_and_validate kong 'curl -f http://localhost:8001/status'
          ;;
        studio|rest)
          docker compose restart "$svc"  # Pas de healthcheck simple, relance suffit
          ;;
      esac
    fi
  done

  # Validation globale post-fix
  sleep 30
  local final_unhealthy=$(get_unhealthy_services)
  if [[ -n "$final_unhealthy" ]]; then
    warn "Services persistants unhealthy: $final_unhealthy - Consultez $LOG_FILE et relancez si besoin."
  else
    ok "Tous services healthy aprÃ¨s fix!"
  fi

  log "ðŸ“‹ Logs diagnostics: $LOG_FILE"
  log "ðŸ”„ Pour relancer tout: docker compose down && docker compose up -d"
}

# ExÃ©cution main
main_fix "$@"